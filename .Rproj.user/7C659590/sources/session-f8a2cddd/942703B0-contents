---
knit: "bookdown::render_book"
---

\appendix

# Appendix  

```{r, echo=FALSE, eval=FALSE}
citation("tidyverse")
citation("dplyr")
citation("fpp3")
citation("mvtnorm")
citation("gridExtra")
version$version.string
```

All codes are performed in R Statistical Software (version 4.2.1 (2022-06-23)). The packages used are `tidyverse` [@tidy19], `dplyr` [@dplyr23], `fpp3` [@fpp23], `gridExtra` [@gridExtra], and `mvtnorm` [@GBMMLSH21].


## Model Specification

The error term, $\epsilon_t$, in each model is assumed to be independent and normally distributed with a zero mean and a constant variance. Each model is independent. Even if using the same notation for unknown parameters across models, the estimators are different. The index $t$ takes the values from $1$ to the total sample size $T$.

Exact formulas and explanations of these models can be found in @fpp3. The formula of the conditional variance for the ETS(M,N,N) model is discussed in Chapter 6.3 of @HKOS08.

### Nonstationary S\&P 500 Index

1. ARIMA(1,1,1) model with an intercept of the natural logarithm of S\&P 500 index. 
\begin{equation*}
log(y_t) = c + log(y_{t-1}) + \phi_1\big[log(y_{t-1})-log(y_{t-2})\big] + \epsilon_t + \theta_1\epsilon_{t-1}
\end{equation*}

2. ETS(M,N,N) model of the natural logarithm of S\&P 500 index. 
\begin{align*}
log(y_t) &= \ell_{t-1} (1+\epsilon_t) \\
\ell_t &= \ell_{t-1} (1+\alpha \epsilon_t) \\
\end{align*}

3. A classical linear regression model of the natural logarithm of the S\&P 500 index and ARIMA(1,0,0) errors. 
\begin{align*}
log(y_t) &= \beta_0 + \beta_1 t + u_t \\
u_t &= \phi_1 u_{t-1} + \epsilon_t
\end{align*}



### Stationary S\&P 500 Index

1. ARMA(1,1) model with an intercept of the natural logarithm of S\&P 500 returns. 
\begin{equation*}
log(y_t) - log(y_{t-1}) = c + \phi_1\big[log(y_{t-1})-log(y_{t-2})\big] + \epsilon_t + \theta_1\epsilon_{t-1}
\end{equation*}

2. A classical linear regression model of the natural logarithm of the S\&P 500 returns and ARMA(1,1) errors. 
\begin{align*}
log(y_t) - log(y_{t-1}) &= \beta_0 + u_t \\
u_t &= \phi_1 u_{t-1} + \epsilon_t + \theta_1\epsilon_{t-1}
\end{align*}



### Well-specified Models for Seasonal Unemployment Dataset

1. ARIMA(2,0,2)(0,1,1)[4] model with an intercept of the natural logarithm of unemployed individuals.
\begin{align*}
log(y_t) &= c + log(y_{t-4}) + \phi_1\big[log(y_{t-1})-log(y_{t-5})\big] + \phi_2\big[log(y_{t-2})-log(y_{t-6})\big] \\
         &+ \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + \Theta_1\epsilon_{t-4} + \theta_1\Theta_1\epsilon_{t-5} + \theta_2\Theta_1\epsilon_{t-6} \\
\end{align*}


2. ETS(A,A,A) model of the natural logarithm of unemployed individuals. 
\begin{align*}
log(y_t) &= \ell_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t \\
\ell_t &= \ell_{t-1} + b_{t-1} + \alpha \epsilon_t \\
b_t &= b_{t-1} + \beta \epsilon_t \\
s_{t} &= s_{t-m} + \gamma \epsilon_t
\end{align*}



### Poorly-specified Models for Seasonal Unemployment Dataset

1. ARIMA(2,1,0) model with an intercept of the natural logarithm of unemployed individuals.
\begin{equation*}
log(y_t) = c + log(y_{t-1}) + \phi_1\big[log(y_{t-1})-log(y_{t-2})\big] + \phi_2\big[log(y_{t-2})-log(y_{t-3})\big] + \epsilon_t
\end{equation*}

2. ETS(A,A,N) model of the natural logarithm of unemployed individuals. 
\begin{align*}
log(y_t) &= \ell_{t-1} + b_{t-1} + \epsilon_t \\
\ell_t &= \ell_{t-1} + b_{t-1} + \alpha \epsilon_t \\
b_t &= b_{t-1} + \beta \epsilon_t
\end{align*}





## Optimal Weight Derivation Details {#detail}

In this section, we detail the derivation steps of producing the results in Section \@ref(op).

Recall that the data is drawn from the true DGP:
\[y_i = \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i, \ \ \epsilon_i \stackrel{i.i.d}{\sim} N(0,\sigma^2_{\epsilon}) \ \ \ (i = 1,2,...,N),\]
where the in-sample period (R) is used to estimate the parameters for the following two constituent models:
\begin{align*}
M_1: y_i &= \alpha_1 x_{1i} + u_{1i}, \ \ u_{1i} \stackrel{i.i.d}{\sim} N(0,\sigma^2_1) \\
M_2: y_i &= \alpha_2 x_{2i} + u_{2i}, \ \ u_{2i} \stackrel{i.i.d}{\sim} N(0,\sigma^2_2).
\end{align*}

Besides, we allow $x_{1i}$ and $x_{2i}$ to have a small correlation, otherwise, there may be multicollinearity, resulting in higher standard errors of estimated parameters.

For simplicity, these models can be written in matrix form
\begin{align*}
y &= x_1 \beta_{1} + x_2 \beta_{2} + \epsilon \\
M_1 : y &= x_1 \alpha_{1} + u_1 \\
M_2 : y &= x_2 \alpha_{2} + u_2,
\end{align*}
where
\[
     {y}=\begin{bmatrix}
           y_{1} \\
           y_{2} \\
           \vdots \\
           y_{N}
         \end{bmatrix},\;
     {x_1}=\begin{bmatrix}
           x_{11} \\
           x_{21} \\
           \vdots \\
           x_{N1}
         \end{bmatrix},\;
    {x_2}=\begin{bmatrix}
           x_{12} \\
           x_{22} \\
           \vdots \\
           x_{N2}
         \end{bmatrix},\;
    {\epsilon}=\begin{bmatrix}
           \epsilon_{1} \\
           \epsilon_{2} \\
           \vdots \\
           \epsilon_{N}
         \end{bmatrix}.
\]

Applying the OLS estimation, we can immediately obtain the formula of $\hat\alpha_{1}$ and $\hat\alpha_{2}$. Given a weak correlation between regressors, each formula will have an extra component, which represents that correlation.
\begin{align*}
    \hat\alpha_{1} &= (x'_1x_1)^{-1} x'_1y \\
    &= (x'_1x_1)^{-1} x'_1(x_1 \beta_1 + x_2 \beta_2 + \epsilon) \\
    &= \beta_1 + (x'_1x_1)^{-1} x'_1x_2 \beta_2 \\
    &= \beta_1 + var(x_1)^{-1} cov(x_1,x_2) \beta_2 \\
    \\
    \hat\alpha_{2} &= (x'_2x_2)^{-1} x'_2y \\
    &= (x'_2x_2)^{-1} x'_2(x_1 \beta_1 + x_2 \beta_2 + \epsilon) \\
    &= \beta_2 + (x'_2x_2)^{-1} x'_2x_1 \beta_1 \\
    &= \beta_2 + var(x_2)^{-1} cov(x_2,x_1) \beta_1 \\
\end{align*}
\begin{align*}
    \hat y_{\omega} &= \hat y_1 \omega + \hat y_2 (1-\omega) \\
    &= x_1 \hat\alpha_1 \omega + \ x_2 \hat\alpha_2 (1-\omega) \\
    &= x_1 \hat\alpha_1 \omega - x_2 \hat\alpha_2 \omega + x_2 \hat\alpha_2 \\
    &= (x_1 \hat\alpha_1 - x_2 \hat\alpha_2) \omega + x_2 \hat\alpha_2 
\end{align*}

\begin{align*}
\hat{\omega}_{\text{opt}} 
&= \underset{\omega \in [0,1]}{\arg\min} \ \frac{1}{R} \sum^R_{t=1} \big(y - \hat y_{\omega}\big)' \big(y - \hat y_{\omega}\big) \\
&= \underset{\omega \in [0,1]}{\arg\min} \ \frac{1}{R} \sum^R_{t=1} \big[y-(x_1 \hat\alpha_1 - x_2 \hat\alpha_2) \omega - x_2 \hat\alpha_2\big]'\big[y-(x_1 \hat\alpha_1 - x_2 \hat\alpha_2) \omega - x_2 \hat\alpha_2\big]\\
\end{align*}
\begin{align*}
    -2(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (y-(x_1 \hat\alpha_1 - x_2 \hat\alpha_2) \hat\omega_{opt} - x_2 \hat\alpha_2) &= 0 \\
    (x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (x_1 \hat\alpha_1 - x_2 \hat\alpha_2) \hat\omega_{opt} &= (x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (y - x_2 \hat\alpha_2) \\
    \hat\omega_{opt} &= \frac{(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (y - x_2 \hat\alpha_2)}{(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (x_1 \hat\alpha_1 - x_2 \hat\alpha_2)} \\
    \hat\omega_{opt} &= \frac{(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (y - x_2 \hat\alpha_2)}{(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' (x_1 \hat\alpha_1 - x_2 \hat\alpha_2)} \\
    \hat\omega_{opt} &= \frac{(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' y - (x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' x_2 \hat\alpha_2}{\hat\alpha'_1 x'_1 x_1 \hat\alpha_1 - 2\hat\alpha'_1 x'_1 x_2 \hat\alpha_2 + \hat\alpha'_2 x'_2 x_2 \hat\alpha_2} \\
\end{align*}






### Formula related to the $F$-statistics

To clearly see the relationship between the in-sample fit and the optimal weight, equation \ref{eqn:opt} can be linked with the F-statistics of two models. The F-test of overall significance is a formal hypothesis test, which examines the explanatory power of the whole model.


The hypothesis of the overall significance test for $M_1$ can be written as $H_0: R\alpha_1 = r$ and $H_1: R\alpha_1 \ne r$ where $R$ is a scalar 1 (or an identity matrix when $\alpha$ is a column vector) and $r$ is a scalar 0 (or a column vector of 0).

Define $m$ as the number of restrictions in the null hypothesis, and the sum squared of errors (SSE) for the full (true) model \ref{eqn:DGP} is $SSE_{full} = (y - x_1 \hat\beta_1 - x_2 \hat\beta_2)'(y - x_1 \hat\beta_1 - x_2 \hat\beta_2)$. Then the unbiased estimator of the true model variance is $s^2=\frac{SSE_{full}}{R-2}$.

The F-statistic follows a F-distribution with degrees of freedom (1,R-2) under $H_0$, which is defined as
\begin{align*}
F_{\alpha_1} &= (R\hat\alpha_1 - r)'[s^2R(x_1'x_1)^{-1}R']^{-1}(R\hat\alpha_1 - r)/m \\
&= (\hat\alpha_1 - 0)' \Big[s^2(x_1'x_1)^{-1}\Big]^{-1} (\hat\alpha_1 - 0)/1 \\
&= R \ s^{-2} \ \hat\alpha'_1 \text{cov}_R(x_1,x_1) \hat\alpha_1. \\
\end{align*}


Similarly, we have 
\begin{equation*}
F_{\alpha_2} = R \ s^{-2} \ \hat\alpha'_2 \text{cov}_R(x_2,x_2) \hat\alpha_2 \sim F_{1,R-2} \text{ under  H}_0.
\end{equation*}


The optimal weight can also be constructed by the F-statistics of $M_1$ and $M_2$.

\begin{equation*}
\hat\omega_{opt} = \frac{F_{\alpha_1}- R \ \hat\alpha_1'\text{cov}_R(x_1,x_2)\hat\alpha_2/s^2}{F_{\alpha_1} + F_{\alpha_2} - 2 R \ \hat\alpha_1'\text{cov}_R(x_1,x_2)\hat\alpha_2/s^2}.
\end{equation*}

If the covariance between $x_1$ and $x_2$ is close to zero, the optimal weight can be approximated as $\hat\omega_{opt} = \frac{F_{\alpha_1}}{F_{\alpha_1} + F_{\alpha_2}}$. This is the reason why the in-sample performance of model is highly correlated with the presence of the puzzle. 



