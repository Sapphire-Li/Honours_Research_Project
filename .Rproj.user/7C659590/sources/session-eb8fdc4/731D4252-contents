---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction 

## Research Question and Objective

This paper aims to demonstrate the presence of the forecast combination puzzle in various settings, besides the time series domain, and to empirically examine a general solution to the forecast combination puzzle. The combination puzzle refers to the extensive empirical finding that the simple average combination method often outperforms sophisticated combination methods. Over the past 50 years, the empirical study undertaken so far has been limited, in that most attention has been focused on different time series datasets. Thus, it is necessary to explore whether the forecast combination puzzle is present in other data types. Furthermore, the theoretical general solution for the forecast combination puzzle still lacks empirical support. This study will, therefore, extend to examine the application of the general explanation and solution proposed by @ZMFP22 and @FZMP23 through a forecasting accuracy test.



## Motivation

Forecasting accuracy is always a concern when forecasts are used in decision-making. Under the classical frequentist approach, forecasters often choose only one "best model" to mimic the actual data generating process of the target variable and then use it to predict future values. However, that single model could be misleading, as it may not capture all the important features of the data. The idea of combining multiple estimates of unknown interests already exists before the well-known seminal work presented by @BG69. They popularized the use of forecast combination for optimal forecasts with several combination techniques. The dramatic improvements in forecast accuracy, along with flexible combination methods, have attracted increasing attention and contributions from researchers in different fields, both theoretical and empirical [@C89;T06]; see @WHLK22 for a modern literature review over the past 50 years.  



In short, forecast combinations involve producing point or density forecasts and then combining them based on a rule or weighting scheme. This process can incorporate more independent and unique characteristics of the true data generating process to mitigate different sources of uncertainty. However, issues could arise with arbitrary or careless implementation. One surprising phenomenon in many empirical studies, coined by @SW04, is the so-called "forecast combination puzzle": "theoretically sophisticated weighting schemes should provide more benefits than the simple average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22] (see section 2.6 for more details and examples). This counter-intuitive result is widely discovered in time series settings, what will happen when working with datasets such as surveys of professional forecasters, dynamic panels, and pure cross-sectional?  



Lastly, a general solution to explaining the forecast combination puzzle is still under development and lacks public acceptance. If the forecast combination puzzle occurs in every setting, it is essential to explore its cause and support the theory with empirical evidence. @FZMP23 demonstrated that, in theory, the cause of the puzzle is the way researchers produce forecast combinations, called a "two-step" approach in the paper. The constituent model forecasts are determined first with estimated parameters, and the unknown weights are then estimated conditional on all the estimates. Due to the unawareness and dimensionality of combining all unknown parameters, this two-step approach is commonly studied and used in the literature, e.g., HM07; @GA11; @GR13; BS16. @FZMP23 further claims that the forecast combination puzzle can be avoided when unknown parameters and weights are estimated in one step, namely a "one-step" approach, when feasible. In other words, if forecasts are produced by estimating parameters and weights simultaneously, the sophisticated weighting schemes should (asymptotically) be superior. This new finding relies on the investigation of forecast combination performance conducted by @ZMFP22 in terms of the one- and two-step approaches. In this paper, I will use some real data to empirically support all their ideas, along with a measure of forecast accuracy test, which examines the null hypothesis of *no inferior forecast performance*.  









