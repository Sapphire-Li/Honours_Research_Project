---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction 

## Research Objective

This thesis aims to investigate the determinants behind, and evidence for the forecast combination puzzle in various domains, and to empirically examine a general solution to the forecast combination puzzle. The combination puzzle refers to the well-known empirical finding that an equally weighted combination of forecasts generally outperforms more sophisticated combination schemes. This phenomenon is often found in the point combinations but it also appears in the density combinations. In this research, we will work with the density forecasts as point forecasts are implicitly included and they offer forecasters a more comprehensive view. Over the past 50 years, the empirical studies undertaken so far have focused more on different time series settings. Thus, one of the main contributions of this research will be to investigate the presence of the combination puzzle in settings outside of pure time series models with density forecasts. As an additional contribution, we will assess the veracity, and applicability, of a recently proposed solution to the forecast combination puzzle suggested in @ZMFP22 and @FZMP23. 



## Literature Review and Motivation

The accuracy of forecasts is of critical concern for forecasters and decision makers. An idea of combining multiple forecasts from different models was originally proposed in the seminal work of @BG69. With the evidence of dramatic improvements in the forecast accuracy, forecast combinations have attracted increasing attention and contributions in the literature, both theoretical and applied [@C89;@T06]. In short, forecast combination methods involve producing point or density forecasts and then combining them based on a rule or weighting scheme. This process can sometimes capture more meaningful characteristics of the true data generating process than using a single model, and allow us to combine the best features of different models within a single framework. Researchers have examined a variety of combination methods for both point and density forecasts over the past 50 years, see @WHLK22 for a modern literature review.  



In most time series setting under which forecast combinations are employed, a striking empirical phenomenon is often observed, coined by @SW04, as the "forecast combination puzzle". The puzzle is encapsulated by the fact that "theoretically sophisticated weighting schemes should provide more benefits than the sample average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22]. In the literature, there are two possible explanations for the puzzle. One concentrates on the estimation uncertainty in combination weight (@SW98, @SW04 and @SW09). Complicated weighting schemes introduce variability when estimating parameters whereas the simple averaging does not require any estimation. On the other hand, @E11 and @CMVW16 explore the trade-off between bias and variance in the Mean Squared Forecast Error (MSFE). They proved that equally weighted combination is unbiased and its variance has only one component, resulting in a smaller mean squared error than a biased combination. However, this is mainly applicable to the MSFE scheme. Recently, @ZMFP22 and @FZMP23 proposed a new explanation for the puzzle in a general way by investigating the sampling variability of the forecasts induced via estimation of the constituent model forecasts (i.e., the models used to produce the forecasts). They illustrated that, asymptotically, the bias and variability mainly come from the estimation of the models used to produce the constituent model forecasts.



The goal of this thesis is two-fold: first, to search for empirical evidence of the combination puzzle in settings outside of the usual time series in which it has been found; second, to test the empirical veracity of the theoretical solution to the puzzle found in @FZMP23, both within, and outside of, the standard time series setting where the puzzle is often observed.

















## Research Objective

This thesis aims to investigate the determinants behind, and evidence for the forecast combination puzzle in various domains. The combination puzzle refers to the well-known empirical finding that an equally weighted combination of forecasts generally outperforms more sophisticated combination schemes. While this phenomenon is often referenced in the point forecast combinations literature, it is also present in the literature on density forecast combinations. Starting with two different types of time series datasets, several two-model pools are constructed to explore how the presence of the puzzle is correlated with the in-sample performance of the constituent models. 

The empirical studies undertaken so far have focused more on pure time series settings, while there is little literature on the puzzle in the cross-sectional setting. A simulated study is designed to investigate the puzzle in the two-model pool under a regression analysis. In addition, we can derive and obtain a closed-form expression to support findings in the simple regression case. Throughout, we measure the performance of density combinations via the log score function and use mean squared forecast error to assess the accuracy of point combinations.





## Literature Review and Motivation

Forecast accuracy is of critical concern for forecasters and decision makers. The application of forecast combination, originally proposed in the seminal work of @BG69, provides the evidence of dramatic improvements in forecast accuracy, and therefore has attracted wide attention and contributions in the literature, both theoretical and applied [@C89;@T06]. More importantly, this approach often has robust performance across various types of data, proved by numerous empirical results [@GA11]. A prominence of researchers also devote efforts on probabilistic forecasting to obtain more information about the uncertainty of the resulting forecast. Similar to point forecasts, researchers now found that density forecast combination outperforms individual density forecast [e.g., @HM07;@GA11].



Forecast combination methods, in general, involve combining multiple forecasts generated from individual or constituent models based on a rule or weighting scheme. Every scheme has its own objective function for producing the "best" forecast combination, along with the optimal weight assigned to each model. This process can sometimes capture more meaningful characteristics of the true data generating process than using a single model, and allows us to combine the best features of different models within a single framework. Researchers have examined a variety of combination methods for both point and density forecasts over the past 50 years, see @WHLK22 for a modern literature review.

In most time series setting under which forecast combinations are employed, a striking empirical phenomenon is often observed, coined by @SW04, as the "forecast combination puzzle". The puzzle is encapsulated by the fact that "theoretically sophisticated weighting schemes should provide more benefits than the simple average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22]. In other words, complex weighting schemes are designed to improve in-sample accuracy, so these refined forecast combinations should perform better out-of-sample in theory. However, the mean of the contemporaneous forecasts appears to be more robust in practice than forecasts combined through complicated weighting schemes. This finding has been continuously reaffirmed by extensive literature reviews and papers [e.g., @MACF82; @C89; @MSA18; @MSA20], and the simple averaging naturally becomes a benchmark. 



The literature explains the puzzle mainly in three aspects: the estimation uncertainty in complicated weighting schemes [@SW98; @SW04; @SW09], the bias and inefficiency in the Mean Squared Forecast Error (MSFE) function [@E11; @CMVW16], and the sampling variability of the forecasts induced via estimation of the constituent model forecasts [@ZMFP22; @FZMP23]. However, all of these explanations implicitly assume that the puzzle will be in evidence when combining forecasts, regardless of the choice of constituent models or the weighing scheme. They ignore the possibility that complicated combination methods can perform much better than the simple average in some cases. In order to make more rigorous explanation statement, we systematically explore the determinants behind the presence of the puzzle under both time series and cross-sectional settings.


Consider a simple case of two-model combination, our initial conjecture is that the presence of the puzzle is highly related to the in-sample fit of two constituent models. When constituent models have similar in-sample fit, the puzzle will be in evidence. Otherwise, the presence of the puzzle is uncertain. Intuitively, the model in-sample performance determines the behavior of forecasts, so forecasts produced by two similarly performed models will not differ much, leading the estimated optimal weight to be around a half. Therefore, it is reasonable to use the simple average method given that the mean of two forecasts is a good estimate and, more importantly, the forecast variance will be halved with no extra parameter estimation. Consequently, we should expect a small difference of the forecast accuracy between the simple averaging and the sophisticated weighting scheme, which is known as the forecast combination puzzle. On the contrary, if two models have distinct in-sample fit, the optimal forecast combination will give more weight to the better one and therefore both weights will be far away from a half. The presence of the puzzle now becomes ambiguous because there are two possible situations in this case, one is that the simple average forecast perform much better than the optimal combination forecast, and the other one is the opposite. By definition, the puzzle is apparently evident in the first case and hard to argue in the second case. Table \ref{tab:1} summarizes the hypothesis. We evaluate two constituent models based on their in-sample performance in a relative sense, and also allow models to perform equivalently ``Bad`` for different reasons.

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
                       &      & \multicolumn{2}{c}{$M_2$} \\
                       &      & Good       & Bad       \\
\multirow{2}{*}{$M_1$} & Good & $\surd$    & $?$ \\
                       & Bad  & $?$        & $\surd$
\end{tabular}
\caption{The first row and the first column refer to two constituent models in a combination, $M_1$ and $M_2$. ``Good`` means that the model fits the data well, whereas ``Bad`` denotes that the model fails to capture some important features of the data. The ``$\surd$`` indicates the presense of the forecast combination puzzle, while ``$?$`` implies that the presense of the puzzle is uncertain.}
\label{tab:1}
\end{table}



Even though there is a widespread literature among different pure time series settings, no attention appears to have been given to the cross-sectional setting. We investigate the forecasting performance of two-model pools for simulated cross-sectional data in using simple linear regression models. This study provides sufficient empirical evidence to further examine and support the conjecture in Table \ref{tab:1}. In addition, we find evidence that the forecast combination puzzle is not just about the fit of constituent models but the interaction of the model with the true data generating process (DGP). As an additional contribution, we derive the mathematical relationship between the optimal weight and elements in the true DGP.


While various explanations for the forecast combination puzzle have been suggested over the years (see the above references), a general solution to the puzzle has so far proved elusive. Recently, @ZMFP22 and @FZMP23 proposed a new explanation for the puzzle in a general way by investigating the sampling variability of the forecasts induced via estimation of the constituent model forecasts (i.e., the models used to produce the forecasts). They illustrated that, asymptotically, the bias and variability mainly come from the estimation of the models used to produce the constituent model forecasts. The common way of producing forecast combinations keeps the model estimation uncertainty fixed during the weight estimation process, which is one reason of having the puzzle. @FZMP23 show that if constituent models and weights can be estimated jointly, the puzzle can be eliminated. Under this approach, the sophisticated weighting schemes should (asymptotically) be superior.

The goal of this thesis is two-fold: first, to substantiate the presence of the combination puzzle in the usual time series in which it has been found; second, to explore the relationship between the puzzle and the in-sample fit of constituent models; third, to search for empirical evidence of the combination puzzle in cross-sectional settings; fourth, to test the empirical veracity of the theoretical solution to the puzzle found in @FZMP23, both within, and outside of, the standard time series setting where the puzzle is often observed.
