---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction 

## Research Objective

This thesis aims to investigate the determinants behind, and evidence for the forecast combination puzzle in various domains. The combination puzzle refers to the well-known empirical finding that an equally weighted combination of forecasts generally outperforms more sophisticated combination schemes. While this phenomenon is often referenced in the point forecast combinations literature, it is also present in the literature on density forecast combinations. Starting with two different types of time series datasets, several two-model pools are constructed to explore how the presence of the puzzle is correlated with the in-sample performance of the constituent models. 

The empirical studies undertaken so far have focused more on pure time series settings, while there is little literature on the puzzle in the cross-sectional setting. A simulated study is designed to investigate the puzzle in the two-model pool under a regression analysis. In addition, we can derive and obtain a closed-form expression to support findings in the simple regression case. Throughout, we measure the performance of density combinations via the log score function and use mean squared forecast error to assess the accuracy of point combinations.





## Literature Review and Motivation

Forecast accuracy is of critical concern for forecasters and decision makers. The application of forecast combination, originally proposed in the seminal work of @BG69, provides the evidence of dramatic improvements in forecast accuracy, and therefore has attracted wide attention and contributions in the literature, both theoretical and applied [@C89;@T06]. More importantly, this approach often has robust performance across various types of data, proved by numerous empirical results [@GA11]. A prominence of researchers also devote efforts on probabilistic forecasting to obtain more information about the uncertainty of the resulting forecast. Similar to point forecasts, researchers now found that density forecast combination outperforms individual density forecast [e.g., @HM07;@GA11].



Forecast combinations refer to the idea of combining multiple forecasts generated from individual or constituent models. The forecast combination methods, in general, involve combining these forecasts based on a rule or weighting scheme. Every scheme has its own objective function for producing the "best" forecast combination, along with the optimal weight assigned to each model. This process can sometimes capture more meaningful characteristics of the true data generating process than using a single model, and allows us to combine the best features of different models within a single framework. Researchers have examined a variety of combination methods for both point and density forecasts over the past 50 years, see @WHLK22 for a modern literature review.

In most time series setting under which forecast combinations are employed, a striking empirical phenomenon is often observed, coined by @SW04, as the "forecast combination puzzle". The puzzle is encapsulated by the fact that "theoretically sophisticated weighting schemes should provide more benefits than the simple average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22]. In other words, complex weighting schemes are designed to improve in-sample accuracy, so these refined forecast combinations should perform better out-of-sample in theory. However, the mean of the contemporaneous forecasts appears to be more robust in practice than forecasts combined through complicated weighting schemes. This finding has been continuously reaffirmed by extensive literature reviews and papers [e.g., @MACF82; @C89; @MSA18; @MSA20], and the simple averaging naturally becomes a benchmark. 



The literature explains the puzzle mainly in three aspects: the estimation uncertainty in complicated weighting schemes [@SW98; @SW04; @SW09], the bias and inefficiency in the Mean Squared Forecast Error (MSFE) function [@E11; @CMVW16], and the sampling variability of the forecasts induced via estimation of the constituent model forecasts [@ZMFP22; @FZMP23]. However, all of these explanations implicitly assume that the puzzle will be in evidence when combining forecasts, regardless of the choice of constituent models or the weighing scheme. They ignore the possibility that complicated combination methods can perform much better than the simple average in some cases. In order to make more rigorous explanation statement, this paper systematically explore the determinants behind the presence of the puzzle with both time series and cross-sectional datasets.



Consider a simple case of two-model combination, the initial conjecture is that the presence of the puzzle is highly related to the in-sample fit of two constituent models. When both models fit the in-sample data well or bad, the puzzle is likely to happen. Intuitively, if the two models have comparable in-sample performance, then their forecasts will not differ much, and therefore the mean of two forecasts will not change much. More importantly, the forecast variance will be halved since there is no estimation required for simple average method. On the other hand, the optimal combination weight will not differ much from 0.5 because two models provide similar accuracy according to the in-sample fit. Besides, the sophisticated weighting scheme often involves an extra parameter estimation, which may introduce more variance and cannot be offset by its superior forecast accuracy. Therefore, the simple method is likely to outperform and we fall into the forecast combination puzzle. 

If only one of the models fails to capture the data patterns, the optimal forecast combination will give more weight to the better one. Nevertheless, it is ambiguous as to whether or not the puzzle will be in evidence, with the simple average forecast performing much better than the optimal combination forecast, or vice versa. Actual data are highly flexible and models with unchanging structure are unlikely to adapt to any changing characteristics in the future. A good model that fits the in-sample period outstandingly can either continue maintaining its excellent performance or have an inverse impact on the forecast accuracy over the out-of-sample period. In addition, the optimal combination should give more weight to the better model, which should be weighted far away from 0.5. Therefore, one of these two combination methods should provide more accuracy but there is not enough information to determine which one. Table \ref{tab:1} summarizes this hypothesis.

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
                       &      & \multicolumn{2}{c}{$M_2$} \\
                       &      & Good       & Bad       \\
\multirow{2}{*}{$M_1$} & Good & $\surd$    & $?$ \\
                       & Bad  & $?$        & $\surd$
\end{tabular}
\caption{The first row and the first column refer to two constituent models in a combination, $M_1$ and $M_2$. ``Good`` means that the model fits the data well, whereas ``Bad`` denotes that the model fails capture some important features of the data. The ``$\surd$`` represents the presense of the forecast combination puzzle, while ``$?$`` implies that the presense of the puzzle is uncertain.}
\label{tab:1}
\end{table}

More specifically, when the accuracy of in-sample fit for two models are very similar, then two models are both ``Good`` or both ``Bad``, which are the diagonal cases in Table \ref{tab:1}. It is possible that two models perform equally bad but for different reasons. In terms of the off-diagonal cases, a ``Good`` model fit and a ``Bad`` model fit are determined in a relative sense, i.e., one model fits the data far more superior than the other. This can be indicated by the apparent difference in the in-sample accuracy between two models.

Even though there is a widespread literature among different pure time series settings, no attention appears to have been given to the cross-sectional setting. We investigate the forecasting performance of two-model pools for cross-sectional data in using simple linear regression models in a simulation study. We find evidence that the forecast combination puzzle is not just about the fit of constituent models but the interaction of the model with the true DGP. Elements that influence the relationship are the sample size, the true value of parameters, and the variances of regressors. The advantages of using simulation are that the true DGP is known and we can easily manipulate values to see any interesting changes. This study also provides sufficient empirical evidence to better examine or support our conjecture in Table \ref{tab:1}. In addition, the regression form is easy to understand and analyse how various parts interact with each other. For example, it is known that the in-sample fit of a linear regression model can be represented by $R^2$, which is therefore a suitable measure to determine ``Good`` and ``Bad`` models in this context.



While various explanations for the forecast combination puzzle have been suggested over the years (see the above references), a general solution to the puzzle has so far proved elusive. Recently, @ZMFP22 and @FZMP23 proposed a new explanation for the puzzle in a general way by investigating the sampling variability of the forecasts induced via estimation of the constituent model forecasts (i.e., the models used to produce the forecasts). They illustrated that, asymptotically, the bias and variability mainly come from the estimation of the models used to produce the constituent model forecasts. The common way of producing forecast combinations keeps the model estimation uncertainty fixed during the weight estimation process, which is one reason of having the puzzle. @FZMP23 show that if constituent models and weights can be estimated jointly, the puzzle can be eliminated. Under this approach, the sophisticated weighting schemes should (asymptotically) be superior.

The goal of this thesis is two-fold: first, to substantiate the presence of the combination puzzle in the usual time series in which it has been found; second, to explore the relationship between the puzzle and the in-sample fit of constituent models; third, to search for empirical evidence of the combination puzzle in cross-sectional settings; fourth, to test the empirical veracity of the theoretical solution to the puzzle found in @FZMP23, both within, and outside of, the standard time series setting where the puzzle is often observed.







