---
chapter: 3
knit: "bookdown::render_book"
---


# Empirical Results

## Pure time series setting (S\&P 500)  

Reconsidering the example in Section 3 of @GA11, the data we use is the daily Standard and Poor's (S\&P) 500 index from February 11, 2013 to February 10, 2023 (10 years in total), retrieved from the @SP500. The S&P500 index dataset has a total of 2519 ($T$) observations and is partitioned into two periods with a rough proportion. The in-sample period contains the first 60% of the data ($R = 1511$), which is used to estimate all unknown parameters, including the optimal weight. The remaining 40% ($P = 1008$) becomes the out-of-sample period to evaluate the forecast performance.  

We will investigate the presence of the forecast combination puzzle when both models fit the training set well and when one of the model badly fit the data. Constituent models are based on common classes of models: autoregressive integrated moving average (ARIMA), exponential smoothing (ETS), and linear regression model with ARIMA errors. Detailed model specifications for each case will be clarified in the corresponding section.  

We choose three prediction models to study the performance of density predictions across sets of ``two-model`` pools. Each of the $j$ predictive model has a conditional Gaussian density, which takes the form $f^{(j)}(y)=f_j(y_t|\mathcal{F}_{t-1})=N\{y_t; \mu_j, \sigma^2_j\}$, where $N\{x; \mu, \sigma^2\}$ denotes the normal probability density function evaluated at value $x$ with mean $\mu$ and variance $\sigma^2$. The notation $\mathcal{F}_{t-1}$ denotes all information available at time $t-1$, and we assume that the conditional mean and variance of the models are, up to unknown parameters, known at time $t-1$.



### Nonstationary time series

To reduce the level of variability, we take a natural logarithm of the S\&P 500 index $y_t$ and fit the data directly without removing its stochastic trend with three candidate models.

1. ARIMA(1,1,1) model with an intercept of the natural logarithm of S\&P 500 index. 
\begin{equation*}
log(y_t) = c + log(y_{t-1}) + \phi_1\big[log(y_{t-1})-log(y_{t-2})\big] + \epsilon_t + \theta_1\epsilon_{t-1}
\end{equation*}

2. ETS(M,N,N) model of the natural logarithm of S\&P 500 index. 
\begin{align*}
log(y_t) &= \ell_{t-1} (1+\epsilon_t) \\
\ell_t &= \ell_{t-1} (1+\alpha \epsilon_t) \\
\end{align*}

3. A classical linear regression model of the natural logarithm of the S\&P 500 index and ARIMA(1,0,0) errors. 
\begin{align*}
log(y_t) &= \beta_0 + \beta_1 t + u_t \\
u_t &= \phi_1 u_{t-1} + \epsilon_t
\end{align*}

The error term, $\epsilon_t$, in each model is assumed to be independent and normally distributed with a zero mean and a constant variance.  

There are three sets of two-model combinations in total. Consider the weight $w$ takes a value from 0 to 1 and changes by 0.01 every time. The log score, as a function of weight, is generated to search for the optimal weight over the in-sample $R$ period (refer to the top row of Figure \ref{fig:nonstat}). According to equation \ref{eqn:LS2}, the estimated optimal weight corresponds to the maximum point of the curve. Then we can calculate the log predictive score of the optimal combination for the out-of-sample period based on equation \ref{eqn:LS3}.

\begin{table}[ht]
  \centering
  \caption{Density forecasts combination under two-model pools of S\&P 500}
    \begin{tabular}{lccc}
    \toprule
                 & ARIMA(1,1,1)       & ETS(M,N,N)         & LR \\
    \midrule
    ARIMA(1,1,1) & \textit{2345.9262} & 2441.7832          & 2362.7701 \\
    ETS(M,N,N)   & 0.65               & \textit{2442.2965} & 2423.2140 \\
    LR           & 0.41               & 0.21               & \textit{2338.6005} \\
    \bottomrule
    \multicolumn{4}{l}{\footnotesize The diagonal entries contains individual log predictive score calculated with $R$ observations.}\\
    \multicolumn{4}{l}{\footnotesize The log predictive scores of optimal combinations with $R$ observations are located above the diagonal.}\\
    \multicolumn{4}{l}{\footnotesize Entries below diagonal show the estimated optimal weight of the model in that column.}\\
    \end{tabular}
  \label{tab:2}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{figures/SP500_nonstationary.pdf}
\caption{Log predictive score of S\&P500 index predictive densities in two-model pools over the in-sample (top) and out-of-sample (bottom) period. Constituent prediction models described in the title. The x-axis represents the weight assigned on the former model of the combination and the y-axis indicates the log predictive score. The orange dot represents the optimal set of weights and the corresponding log predictive score in each case, while the blue dot indicates the forecast performance of the simple averaging method. The green dot, as a reference, refers to the maximum point of the out-of-sample curve.}
\label{fig:nonstat}
\end{figure}

Figure \ref{fig:nonstat} suggests that the forecast combination puzzle is only evident in the second predictive density combination (ARIMA(1,1,1) and Linear Regression). In the middle column, the log predictive score of the optimal combination, 2361.5974, is very close to that of the simple average combination, 2362.7701. However, the other two combinations do not reveal the puzzle. Besides, the simple average performs superior than the optimal combination in both cases. 


\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{figures/log_linegraph.pdf}
\caption{The black vertical line separates the traning set and the evaluation set. The training set is on the left and the evaluation set is on the right.}
\label{fig:llg}
\end{figure}



WHICH ONE???

ETS too bad?

One possible explanation could be that the ETS model badly fits the training set but forecasts well for the evaluation set in both cases. Given this data split, the optimal weight set gives less weight on the ETS model, leading to a low log predictive score for the optimal predictive density combination, compared with the simple average. This phenomenon implies that the optimal weight set could not be fixed but a varying variable when one of the constituent models does not fit well with the data. We conjecture that there is a relationship between the forecast combination puzzle and the fitness of constituent models.





ARIMA & LR too good?

Since we are modelling the log of S\&P500, its trending behavior and structural breaks are dominant and need to be taken care of when fitting the model. That means a good model should capture the trend, deal with the structural breaks and incorporate all important features of this nonstationary data. 

One possible explanation could be that the ARIMA model and the linear regression model fit the training set too well to take any future changes into consideration. It is also unanticipated to incorporate the large structural break and the changing slopes of the trend in the evaluation set as shown in Figure \ref{ig:llg}. Thus, their density forecast combinations end up with lower log predictive scores, compared with the simple average. 








### Stationary time series

Continuing with the same dataset, we now fit the stationary series by taking a first difference of the log of S\&P500. A series is said to be stationary when it has constant mean and variance, and its covariance depends on the time interval only. In other words, the entire series should have a roughly consistent pattern. Then, the training set and the evaluation set will not have completely different behaviors that highly influence the goodness-of-fit of models.

Consider two candidate models: a Gaussian ARMA(1,1) model and a classical linear regression model with ARMA(1,1) errors. Figure \ref{fig:stat} shows that two constituent models have a very similar in-sample accuracy and the puzzle is obvious in the forecast combination. As expected, since both models fit the data well, the simply averaged point forecast performs almost the same as the optimal combined point forecast, indicating the presence of the forecast combination puzzle.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{figures/SP500_stationary.pdf}
\caption{Log predictive score of S\&P500 index predictive densities in two-model pools over the in-sample (left) and the out-of-sample (right) period. The x-axis represents the weight assigned on the ARMA(1,1) model and the y-axis indicates the log predictive score. The meanings of colored dots remain the same.}
\label{fig:stat}
\end{figure}









## Pure time series with seasonality

With the purpose of corroborating above conjectures, we experiment with a quarterly dataset to observe the relationship between the forecast combination puzzle and the specification of models. More specifically, we would like to investigate cases when models are both correctly specified or both misspecified. To make our life easier, we produce point forecasts and evaluate point combinations with Mean Squared Forecast Error (MSFE).

The data considered is the quarterly total number of unemployed individuals (in thousands) from 1985 Q1 to 2023 Q1, retrieved from the Australia Bureau of Statistics [@ABS]. It has a total of 153 ($T$) observations and is slit into two sets in proportion. Same as before, the first 60% of the data ($R = 91$), as the in-sample period, is used to estimate all unknown parameters. The rest 40% ($P = 62$) is the out-of-sample period for the forecast performance evaluation. Also, we use the natural logarithm of the total number of unemployment to reduce the level of variability in the seires.



### Correctly specified models

To ensure compatibility with seasonal component, we propose the Seasonal ARIMA (SARIMA) model and the ETS model: ARIMA(2,0,2)(0,1,1)[4] with drift and ETS(A,A,A). The SARIMA is simply an ARIMA model with extra seasonal component. The first parenthesis is same as before. The second parenthesis represents the seasonal AR, integrated, and MA components respectively, separately by the comma. The number in the box bracket indicates the number of observations per year, i.e., the seasonal frequency. An intercept is included in the model. In the ETS model, the seasonal part is reflected by `S` and the third position in the parenthesis. Due to the log transformation, we have additive error, additive trend, and additive seasonality.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{figures/EMPL_correct.pdf}
\caption{MSFE of predictive points in correctly specified two-model pools over the in-sample (left) and out-of-sample (right) period. The x-axis represents the weight assigned on the SARMA model and the y-axis indicates the value of MSFE. The meanings of colored dots remain the same.}
\label{fig:sdc}
\end{figure}

The forecast combination puzzle is evident in Figure \ref{fig:sdc}. The optimal forecast point combination has a MSFE of 0.000177 and the simple averaging forecast has a MSFE of 0.000178. The difference between them is negligible. From the in-sample combination accuracy plot, we can assert that both models fit the training set equally well, leading the estimated optimal weight to be around 0.5. These results exemplify that producing forecasts with correctly specified models in a two-model pool will lead to the forecast combination puzzle.



### Misspecified models

One way of proposing a `wrong` model for a seasonal dataset is deliberately ignoring the seasonal component in the model specification. Even so, we still try to fit the training set well with SARIMA and ETS models by only blocking the seasonal part: ARIMA(2,1,0) with an intercept and ETS(A,A,N).

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{figures/EMPL_misspecified.pdf}
\caption{MSFE of predictive points in misspecified two-model pools over the in-sample (left) and out-of-sample (right) period. The x-axis represents the weight assigned on the SARMA model and the y-axis indicates the value of MSFE. The meanings of colored dots remain the same.}
\label{fig:sdm}
\end{figure}

Based on the left plot of Figure \ref{fig:sdm}, the ETS model has a bigger mean squared error than the ARIMA model, indicating a poor model performance in fitting the training set. The difference between these misspecified models is much larger than those correctly specified models mentioned above. Furthermore, Figure \ref{fig:sdm} does not reveal any evidence of the puzzle, given the simple average performs superior than the optimal forecast combination. Although the difference of MSFE, 0.000013, seems to be small in magnitude, comparing it with 0.000001 in the correct case, the difference is relatively conspicuous.



