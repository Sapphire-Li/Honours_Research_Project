---
chapter: 
knit: "bookdown::render_book"
---

# Simulation Results

## Pure cross-sectional setting

Given that the forecast combination can greatly improve the forecast accuracy, this idea of model combination can also be applied to the cross-sectional setting. Rather than forecasting future value, cross-sectional data often helps to better understand the individual behavior and decision-making with changing attributes. 

A simulated cross-sectional dataset is designed to study how related elements in the linear regression model affect the presence of the puzzle, as well as the performance of density combinations. In line with previous notations but under the cross-sectional setting, the subscript `t` will change to `i` to represent each individual observation.


CHECK MOTIVATION!!!! 

### Experimental design

The true data-generating process (DGP) is assumed to be a classic linear regression model with only two exogenous and correlated regressors, which satisfies all classical assumptions:

\begin{equation}
\label{eqn:DGP}
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + e_i, \ \ e_i \stackrel{i.i.d}{\sim} N(\mu_e,\sigma^2_e) \\
\end{equation}

where $i$ represents each observation.  

The initial set-up has 15000 (N) artificial cross-sectional observations generated from \ref{eqn:DGP} with $E[x_{1i}] = E[x_{2i}] = 0$, $Var(x_{1i}) = Var(x_{2i}) = 1$, $Cov(x_{1i}, x_{2i}) = 0.7$, $\pmb{\beta} = (\beta_0, \beta_1, \beta_2)' = (1,2,2)'$, $\mu_e = 5$ and $\sigma^2_e=10$.

Following the methodology in Section \@ref(method), the data will be divided into an in-sample period (roughly 60%) for estimation and an out-of-sample period for accuracy evaluation. We propose two misspecified models to generate density forecasts with each only contains one of the regressors. Assume Model 1 includes only $x_{1i}$ as the regressor and the other model, Model 2, includes only $x_{2i}$ as the regressor. The density forecast combinations will follow the construction of `two-model` pools and be evaluated by the log score functions. 


+ Sample size is $N=15000$

+ $E[x_{1i}] = E[x_{2i}] = 0$

+ $Var(x_{1i}) = Var(x_{2i}) = 1$

+ $Cov(x_{1i}, x_{2i}) = 0.7$

+ The true value of $\pmb{\beta} = (\beta_0, \beta_1, \beta_2)' = (1,2,2)'$

+ $\mu_e = 5$ and $\sigma^2_e=10$


\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{figures/Sample_Size_15000.pdf}
\caption{Two curves refer to the in-sample (left) and out-of-sample (right) performance of density combinations with artificial cross-sectional data under the initial set-up. The x-axis represents the weight assigned on Model 1 and the y-axis indicates the log score for each density combination. The orange dot represents the optimal set of weights and the corresponding log predictive score in each case, while the blue dot indicates the forecast performance of the simple averaging method. The green dot, as a reference, refers to the maximum point of the out-of-sample curve.}
\label{fig:ss15000}
\end{figure}

Figure \ref{fig:ss15000} clearly reflects that when the sample size is large enough, the simple average of predicted densities, indicated by the blue dot, can retain the forecast accuracy with a small difference in the log predictive score, compared with the optimal combination indicated by the orange dot. This is an evidence of facing forecast combination puzzle. Given the puzzle, we can change the true value of relevant elements one at a time while holding all others constant, and then summarize the conditions under which the puzzle is likely to be evident.


+ \bf{Sample Size}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/Sample_Size_100-10000.png}
\caption{Three columns refer to cases when $N=100$, $N=1000$, and $N=10000$ respectively while keeping all others constant as the initial set-up. The top graphs represent the in-sample combination performance and the bottom graphs represent the out-of-sample combination accuracy. The meanings of colored dots are the same as those in Figure \ref{fig:ss15000}.}
\label{fig:ss}
\end{figure}

First, it is notable that, in Figure \ref{fig:ss}, the performances of in-sample and out-of-sample combinations have completely different shapes or features when $N=100$ but are gradually similar when $N=1000$ and $N=10000$. In the $N=100$ case, we completely prefer Model 1 to fit the training set, however, the Model 1 becomes the worse choice for the test set. Thus, the averaged density forecast performs much better than the combination recommended by the optimal weight. This implies that the model combination which fits the in-sample well does not necessarily generate better forecasts when the sample size is small. Second, the forecast accuracy of optimal combination and that of simple averaging are getting closer when the sample size increases, which indicates the presence of the forecast combination puzzle. Roughly, the puzzle becomes noticeable when the sample size is larger than 500.

When we have a small dataset, it is not representative of the whole population, so the model estimation involves more randomness and is highly influenced by potential outliers. There is also a possibility of overfitting the training set when the training and test sets have distinct patterns. Therefore, given a large enough dataset and two equally good models, we are very likely to have the puzzle.



+ \bf{Magnitude and Sign of $\pmb{\beta}$}

Next, the sample size is set to be 10000 so that it is large enough to reveal the puzzle. Consider the change in magnitude and sign of $\beta_1$ and $\beta_2$.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/Beta_diff_mag.png}
\caption{In this case, $\beta_1$ and $\beta_2$ have the same sign but different magnitudes. The first column refers to $\beta_1=2$ and $\beta_2=3$, the second column refers to $\beta_1=2$ and $\beta_2=4$, and the first column refers to $\beta_1=2$ and $\beta_2=6$.}
\label{fig:magnitude}
\end{figure}

Based on the results shown in Figure \ref{fig:magnitude}, the puzzle is highly sensitive to the absolute difference between two parameters. If the absolute difference is large enough, generally more than half of the smaller coefficient, it is hard to observe the puzzle and the optimal combination always wins with a higher log predictive score. The larger the absolute difference, the bigger the difference of two log predictive scores.

In the linear regression analysis, the magnitude of each coefficient represents the influence size of each regressor on the dependent variable. A large coefficient means that a change in the corresponding regressor affects the dependent variable more in magnitude. Knowing this, it is reasonable to observe that the Model 1 has a decreasing weight in the optimal combination from left to right in Figure \ref{fig:magnitude}. The effect of $x_2i$ on $y_i$, $\beta_2$, is relatively larger than the effect of $x_1i$ on $y_i$, $\beta_1$, so the Model 2 with $x_2i$ only should be weighted higher in the combination.


\begin{figure}[ht]
\centering
\includegraphics[scale=0.55]{figures/Beta_diff_sign.png}
\caption{$\beta_1$ and $\beta_2$ have the same magnitude but different signs, i.e. $\beta_1=-\beta_2$. The first column considers the case when $\beta_1=2$ and $\beta_2=-2$ and the second column considers the case when $\beta_1=4$ and $\beta_2=-4$.}
\label{fig:sign}
\end{figure}

Figure \ref{fig:magnitude} illustrate that when $\beta_1$ and $\beta_2$ only have opposite signs, the puzzle seems to be insensitive. In both cases, the optimal combination and simple averaging forecast have very similar log predictive scores, which is a strong evidence of the puzzle. Meanwhile, two regressors have the same effect in magnitude on $y_i$, therefore, the weight should be equally assigned in rough. We also notice that the accuracy of the optimal prediction combination can be improved by having higher absolute values of the coefficients. This makes regressors to have larger and more certain impacts on $y_i$, which is substantiated by the above case.



+ \bf{Variance of regressors}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/x_var.png}
\caption{The first column refers to the case when }
\label{fig:xvar}
\end{figure}


larger variance, more variation to explain y, more favored


Variance of the error term will influence the log score value
Pattern is hard to determine






















