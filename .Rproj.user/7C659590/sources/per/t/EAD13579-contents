---
chapter: 3
knit: "bookdown::render_book"
---


# Empirical Results

## Pure time series setting (S\&P 500)  

Reconsidering the example in Section 3 of @GA11, the data we use is the daily Standard and Poor's (S\&P) 500 index from February 11, 2013 to February 10, 2023 (10 years in total), retrieved from the @SP500. The S&P500 index dataset has a total of 2519 ($T$) observations and is partitioned into two periods with a rough proportion. The in-sample period contains the first 60% of the data ($R = 1511$), which is used to estimate all unknown parameters, including the optimal weight. The remaining 40% ($P = 1008$) becomes the out-of-sample period to evaluate the forecast performance.  

We will investigate the presence of the forecast combination puzzle when using correctly-specified models or mis-specified models to fit the data. Constituent models are based on common classes of models: autoregressive integrated moving average (ARIMA), exponential smoothing (ETS), and linear regression model with ARIMA errors. Detailed model specifications for each case will be clarified in the corresponding section.  

We choose $j=1,\cdots,M$, with $M=3$ prediction models to study the performance of density predictions across sets of ``two-model`` pools. Each of the $j$ predictive model has a conditional Gaussian density, which takes the form $f^{(j)}(y)=f_j(y_t|\mathcal{F}_{t-1})=N\{y_t; \mu_j, \sigma^2_j\}$, where $N\{x; \mu, \sigma^2\}$ denotes the normal probability density function evaluated at value $x$ with mean $\mu$ and variance $\sigma^2$. The notation $\mathcal{F}_{t-1}$ denotes all information available at time $t-1$, and we assume that the conditional mean and variance of the models are, up to unknown parameters, known at time $t-1$.



### Nonstationary time series

To reduce the level of variability, we take a natural logarithm of the S\&P 500 index $y_t$ and fit the data directly without removing its stochastic trend with three candidate models.

1. ARIMA(1,1,1) model with an intercept of the natural logarithm of S\&P 500 index. 
\begin{equation*}
log(y_t) = c + log(y_{t-1}) + \phi_1\big[log(y_{t-1})-log(y_{t-2})\big] + \epsilon_t + \theta_1\epsilon_{t-1}
\end{equation*}

2. ETS(M,N,N) model of the natural logarithm of S\&P 500 index. 
\begin{align*}
log(y_t) &= \ell_{t-1} (1+\epsilon_t) \\
\ell_t &= \ell_{t-1} (1+\alpha \epsilon_t) \\
\end{align*}

3. A classical linear regression model of the natural logarithm of the S\&P 500 index and ARIMA(1,0,0) errors. 
\begin{align*}
log(y_t) &= \beta_0 + \beta_1 t + u_t \\
u_t &= \phi_1 u_{t-1} + \epsilon_t
\end{align*}

The error term, $\epsilon_t$, in each model is assumed to be independent and normally distributed with a zero mean and a constant variance.  

There are three sets of two-model combinations in total. Consider the weight $w$ takes a value from 0 to 1 and changes by 0.01 every time. The log score, as a function of weight, is generated to search for the optimal weight over the in-sample $R$ period (refer to the top row of Figure \ref{fig:nonstat}). According to equation \ref{eqn:LS2}, the estimated optimal weight corresponds to the maximum point of the curve. Then we can calculate the log predictive score of the optimal combination for the out-of-sample period based on equation \ref{eqn:LS3}.

\begin{table}[ht]
  \centering
  \caption{Log predictive score of density forecasts combination under two-model pools of S\&P 500}
    \begin{tabular}{lccc}
    \toprule
                 & ARIMA(1,1,1)       & ETS(M,N,N)         & LR \\
    \midrule
    ARIMA(1,1,1) & \textit{2345.9262} & 2441.7832          & 2362.7701 \\
    ETS(M,N,N)   & 0.65               & \textit{2442.2965} & 2423.2140 \\
    LR           & 0.41               & 0.21               & \textit{2338.6005} \\
    \bottomrule
    \multicolumn{4}{l}{\footnotesize The diagonal entries contains individual log predictive score calculated over the out-of-sample period with $P$ observations.}\\
    \multicolumn{4}{l}{\footnotesize The log predictive scores of optimal combinations over the out-of-sample period are located above the diagonal.}\\
    \multicolumn{4}{l}{\footnotesize Entries below diagonal show the estimated optimal weight of the model in that column in the two-model pool.}\\
    \end{tabular}
  \label{tab:2}
\end{table}


\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{figures/SP500_nonstationary.pdf}
\caption{Log predictive score in two-model pools over the in-sample (top) and out-of-sample (bottom) period of density combinations. Constituent prediction models described in the title. The x-axis represents the weight assigned on the former model of the combination and the y-axis indicates the log predictive score. The orange dot represents the optimal set of weights and the corresponding log predictive score in each case, while the blue dot indicates the forecast performance of the simple averaging method. The green dot, as a reference, refers to the maximum point of the out-of-sample curve.}
\label{fig:nonstat}
\end{figure}

Figure \ref{fig:nonstat} suggests that the forecast combination puzzle is only evident in the second predictive density combination (ARIMA(1,1,1) and Linear Regression). In the middle column, the log predictive score of the optimal combination, 2361.5974, is very close to that of the simple average combination, 2362.7701. However, the other two combinations do not reveal the puzzle. Besides, the simple average performs superior than the optimal combination in both cases. 


\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{figures/log_linegraph.pdf}
\caption{The black vertical line separates the traning set and the evaluation set. The training set is on the left and the evaluation set is on the right.}
\label{fig:llg}
\end{figure}



WHICH ONE???

ETS too bad?
One possible explanation could be that the ETS model badly fits the training set but forecasts well for the evaluation set in both cases. Given this data split, the optimal weight set gives less weight on the ETS model, leading to a low log predictive score for the optimal predictive density combination, compared with the simple average. This phenomenon implies that the optimal weight set could not be fixed but a varying variable when one of the constituent models does not fit well with the data. We conjecture that there is a relationship between the forecast combination puzzle and the fitness of constituent models.







ARIMA & LR too good?

Since we are modelling the log of S\&P500, its trending behavior and structural breaks are dominant and need to be taken care of when fitting the model. That means a good model should capture the trend, deal with the structural breaks and incorporate all important features of this nonstationary data. 

One possible explanation could be that the ARIMA model and the linear regression model fit the training set too well to take any future changes into consideration. It is also unanticipated to incorporate the large structural break and the changing slopes of the trend in the evaluation set as shown in Figure \ref{ig:llg}. Thus, their density forecast combinations end up with lower log predictive scores, compared with the simple average. 








### Stationary time series

Continuing with the same dataset, we now fit the stationary series by taking a first difference of the log of S\&P500. A series is said to be stationary when it has constant mean and variance, and its covariance depends on the time interval only. In other words, the entire series should have a roughly consistent pattern. Then, the training set and the evaluation set will not have completely different behaviors that highly influence the goodness-of-fit of models.

Consider two candidate models: a Gaussian ARMA(1,1) model and a classical linear regression model with ARMA(1,1) errors. Figure \ref{fig:stat} shows that two consitutent models have a very similar in-sample accuracy and the puzzle is obvious in the forecast combination. As expected, since both models fit the data well, the simply averaged point forecast performs almost the same as the optimal combined point forecast, indicating the presence of the forecast combination puzzle.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{figures/SP500_stationary.pdf}
\caption{Log predictive score in two-model pools over the in-sample (left) and out-of-sample (right) period of density combinations. The x-axis represents the weight assigned on the ARMA(1,1) model and the y-axis indicates the log predictive score. The meanings of colored dots remain the same}
\label{fig:stat}
\end{figure}









## Pure time series with seasonality

With the purpose of corroborating above conjectures, we experiment with a quarterly dataset to observe the relationship between the puzzle and the specification of models. To make our life easier, we produce point forecasts and evaluate point combinations with Mean Squared Forecast Error.

The data considered is the quarterly total number of unemployed individuals (in thousands) from 1985 Q1 to 2023 Q1, retrieved from the Australia Bureau of Statistics [@ABS].

It has a total of 2519 ($T$) observations and is partitioned into two periods with a rough proportion. The in-sample period contains the first 60% of the data ($R = 1511$), which is used to estimate all unknown parameters, including the optimal weight. The remaining 40% ($P = 1008$) becomes the out-of-sample period to evaluate the forecast performance.  

We will investigate the presence of the forecast combination puzzle when using both mis-specified and correctly-specified models to fit the data. 


There are two combined point forecasts whose MSFEs are calculated over the last P observations



### Misspecified models

With the purpose of examining the case when using two badly fit models, we experiment with seasonal dataset to observe the relationship between the puzzle and the specification of models.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{figures/EMPL_misspecified.pdf}
\caption{}
\label{fig:sdm}
\end{figure}







### Correctly specified models


\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{figures/EMPL_correct.pdf}
\caption{}
\label{fig:sdc}
\end{figure}

























