---
chapter: 
knit: "bookdown::render_book"
---

# Pure Cross-sectional Analysis

Given that the forecast combination can greatly improve the forecast accuracy, this idea of model combination can also be applied to the cross-sectional setting. In this section, a simulated cross-sectional dataset is designed to derive the analytical relationship of the optimal weight in the regression setting and investigate the determinants of the puzzle. In order to get a closed form expression, we work out the formula in the MSE context. A simulation study will then be conducted to evaluate and verify the applicability of findings in point combinations to density combinations. Besides, it is less likely to obtain a closed form expression of the optimal weight using the log scoring rules. 

Compared with the real-life data, implementing simulation is easy to control and make interpretation given that the true DGP is known. Meanwhile, it is an effective way of validating our conjecture by freely changing the elements and looking for the forecast combination puzzle. In line with previous notations but in the cross-sectional setting, the subscript `t` will change to `i` to represent each individual observation.



## Model Setup

The true DGP is assumed to be a linear regression model with no intercept and only two exogenous and weakly correlated regressors, which satisfies all classical assumptions:

\begin{equation}
\label{eqn:DGP}
y_i = \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i, \ \ \epsilon_i \stackrel{i.i.d}{\sim} N(0,\sigma^2_{\epsilon}) \\
\end{equation}

where $i$ represents each observation.  

The forecasting models, or constituent models, are proposed as

\begin{align}
M_1: y_i &= \alpha_1 x_{1i} + e_{1i}, \ \ e_{1i} \stackrel{i.i.d}{\sim} N(0,\sigma^2_1) \\
M_2: y_i &= \alpha_2 x_{2i} + e_{2i}, \ \ e_{2i} \stackrel{i.i.d}{\sim} N(0,\sigma^2_2).
\end{align}


## Optimal Weight (MSE)  {#op}

Following the methodology in Section \@ref(method), the data will be divided into an in-sample period (R) for estimation and an out-of-sample period (P) for accuracy evaluation. As noted before, the estimated optimal weight, $\hat\omega_{opt}$, will be generated over the first R number of observations.



### Formula related to $\hat\alpha$ (and the limit result)

To simplify the notation, we use the matrix form of above linear regression models, and obtain the formula of $\hat\omega_{opt}$ under the MSE weighting scheme

\begin{equation}
\label{eqn:opt}
\hat\omega_{opt} = \frac{(x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' y - (x_1 \hat\alpha_1 - x_2 \hat\alpha_2)' x_2 \hat\alpha_2}{\hat\alpha'_1 x'_1 x_1 \hat\alpha_1 - 2\hat\alpha'_1 x'_1 x_2 \hat\alpha_2 + \hat\alpha'_2 x'_2 x_2 \hat\alpha_2}
\end{equation}

where $\hat\alpha_1$ and $\hat\alpha_2$ are the Ordinary Least Squares (OLS) estimators in $M_1$ and $M_2$ respectively.

A more meaningful expression can be achieved by multiplying both sides with $\frac{1}{R}$ and be simplified as

\begin{equation}
\hat\omega_{opt} = \frac{\hat\alpha_1'\text{cov}_R(x_1,x_1)\hat\alpha_1 - \hat\alpha_1'\text{cov}_R(x_1,x_2)\hat\alpha_2}{\hat\alpha_1' \text{cov}_R(x_1,x_1)\hat\alpha_1 - 2\hat\alpha_1'\text{cov}_R(x_1,x_2)\hat\alpha_2 + \hat\alpha_2'\text{cov}_R(x_2,x_2)\hat\alpha_2}.
\end{equation}


In the classical linear regression setting, the OLS estimator is consistent when the sample size goes to infinity. That is, we should have $\hat\alpha_1 \overset{p}{\to} \alpha_1$ and $\hat\alpha_2 \overset{p}{\to} \alpha_2$. Consider the limit result, we have

\begin{equation}
\label{eqn:limit}
\hat\omega_{opt} \overset{p}{\to} \omega_\star = \frac{\alpha_1'\Sigma_{11}\alpha_1 - \alpha_1'\Sigma_{12}\alpha_2}{\alpha_1'\Sigma_{11}\alpha_1 - 2\alpha_1'\Sigma_{12}\alpha_2 + \alpha_2'\Sigma_{22}\alpha_2}
\end{equation}

where $\omega_\star$ is the true value of the optimal weight, $\Sigma_{jk}$ denotes the variance-covariance matrix of corresponding regressors $x_j$ and $x_k$.

With the limit result \ref{eqn:limit}, we can easily work out the asymptotic determinants of having $\omega_\star=\frac{1}{2}$ and then connect it with the presence of the puzzle. For $\omega_\star=\frac{1}{2}$, it must be that $\alpha_1'\Sigma_{11}\alpha_1 = \alpha_2'\Sigma_{22}\alpha_2$.


Therefore, any situation where this final equality is nearly satisfied will inevitably lead the optimal weight to be around a half. Then, the estimated optimal weight should be close to $\frac{1}{2}$ when the training set is large enough, and then deliver the puzzle as expected. Besides, the equality suggest that the presence of the puzzle is highly relevant to the relative magnitude of $\alpha_1$ and $\alpha_2$, as well as the relative magnitude of $\Sigma_{11}$ and $\Sigma_{22}$. That is, the more similar both of them are, the more likely we will find the puzzle. 

If we assume regressors to have identity variances, then the puzzle will be in evidence when the absolute difference between $\alpha_1$ and $\alpha_2$ is small. Given a small correlation between regressors, $\alpha_1$ and $\alpha_2$ will be close to $\beta_1$ and $\beta_2$ respectively.





## Density Simulations

Applying the learning from Section \@ref(op) to density combinations and log scoring rules where we believe is not likely to get a closed-form expression, we empirically examine the applicability of findings from the point combinations. The initial set-up has 1000 (N) artificial cross-sectional observations generated from the equation \ref{eqn:DGP} with $E[x_{1i}] = E[x_{2i}] = 0$, $Var(x_{1i}) = Var(x_{2i}) = 1$, $Cov(x_{1i}, x_{2i}) = 0.3$, $\pmb{\beta} = (\beta_1, \beta_2)' = (2,2)'$, and $\sigma^2_{\epsilon}=4$. Same as before, around 60% of the data will be used for model estimation. The density forecast combinations will follow the construction of `two-model` pools and be evaluated using the log score. 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{figures/ss_1000.pdf}
\caption{Two curves refer to the in-sample (left) and out-of-sample (right) performance of density combinations with artificial cross-sectional data based on the initial set-up. The x-axis represents the weight assigned on Model 1 and the y-axis indicates the log score for each density combination. The orange dot represents the optimal forecast combination, while the blue dot indicates the forecast performance of the simple average combination.}
\label{fig:ss1000}
\end{figure}

Figure \ref{fig:ss1000} clearly shows that when the sample size is large and two models have similar in-sample performance, the forecast accuracy will be indistinguishable between the simple average of predicted densities and the optimal density forecast combination, which is a strong evidence of the forecast combination puzzle. We can then change the true value of different elements, and determine the conditions under which the puzzle is likely to be evidenced. More rigorously, we will evaluate the in-sample performance with the $R^2$ of constituent models, and then analyse its relationship with the optimal combination weight and the presence of the puzzle.





+ \bf{Sample Size}

\begin{figure}
\includegraphics[width=0.47\textwidth]{figures/ss_50.pdf}
\hspace{\fill}
\includegraphics[width=0.47\textwidth]{figures/ss_100.pdf}
\caption{Two columns refer to the in-sample combination performance (top) and the out-of-sample combination accuracy (bottom) when $N=100$ and $N=1000$ while keeping all others the same as in the initial set-up. The meanings of colored dots are the same as those in Figure \ref{fig:ss1000}.}
\label{fig:samplesize}
\end{figure}

From Figure \ref{fig:samplesize}, it is noticeable that the set of optimal weight varies a lot when we have different sample sizes. Model 1 is given an extremely low weight when $N=50$ whereas it is highly preferred when $N=100$. The optimal weight is 0.48 when the sample size becomes 1000, shown in Figure \ref{fig:ss1000}. Based on the log score curve of in-sample combinations, the optimal weight is highly correlated with the individual model performance. The number of observations can be viewed as one factor that can affect the model fit. Figure \ref{fig:samplesize} also shows that the average density forecast performs much better than the optimal density combination in both cases, i.e., the forecast combination puzzle is found.

\begin{table}[ht]
  \centering
    \begin{tabular}{l|ccc}
    \toprule
    Sample Size      &   $N=50$    &    $N=100$   &  $N=1000$  \\
    \midrule
    $R^2$ of $M_1$   &   0.4928    &    0.4953    &   0.3612   \\
    $R^2$ of $M_2$   &   0.5620    &    0.3320    &   0.3722   \\
    Difference       &   0.0692    &    0.1633    &   0.0110   \\
    Optimal Weight   &    0.07     &     0.98     &    0.48    \\
    Puzzle           &    Yes      &     Yes      &    Yes     \\
    \bottomrule
    \end{tabular}
  \caption{``Difference`` represents the absolute difference of in-sample fit between two models. ``Optimal Weight`` is the estimated weight assigned to $M_1$. ``Puzzle`` indicates whether the simple average is close to or outperforms the optimal forecast combination.}
  \label{tab:size}
\end{table}

Table \ref{tab:size} illustrates that when two models have a relatively big difference in the in-sample fit $R^2$ (in the second and third columns), we are then more likely to have an extreme optimal weight $\omega$. However, when two models have similar $R^2$ in the fourth column, the optimal weight $\omega$ is close to 0.5. These empirical results first support the conjecture that when models have indifferent in-sample fit, the puzzle is likely evidenced. Additionally, they illustrate that the puzzle can be in evidence when one model performs outstandingly.





+ \bf{Magnitude and Sign of $\pmb{\beta}$}

Next, we explore the effect changes in magnitudes or signs of $\beta_1$ and $\beta_2$ given two different sample sizes. From here on, combination plots will be collected and displayed in Appendix \@ref(plot). According to Figure \ref{fig:magnitude}, the puzzle is highly sensitive to the absolute difference between two parameters. If the absolute difference is large enough, generally more than half of the smaller coefficient, it is hard to find the puzzle and the optimal combination always wins with a higher log predictive score. In the linear regression analysis, the magnitude of coefficient represents the impact size of corresponding regressor on the dependent variable. A large value of coefficient means that a change in the regressor will affect the dependent variable more in magnitude. Knowing this, it is reasonable to observe that the Model 1 has a decreasing weight in the optimal combination from left to right in Figure \ref{fig:magnitude}. The effect of $x_{2i}$ on $y_i$, $\beta_2$, is relatively larger than the effect of $x_{1i}$ on $y_i$, $\beta_1$, so the Model 2 with $x_{2i}$ should be weighted higher in the combination.

\begin{table}[ht]
  \centering
    \begin{tabular}{l|cccc}
    \toprule
    Different Magnitudes    &  $\beta_1=2,\ \beta_2=4$   &  $\beta_1=2,\ \beta_2=6$ &  $\beta_1=2,\ \beta_2=4$  &  $\beta_1=2,\ \beta_2=6$  \\
    \midrule
    $R^2$ of $M_1$  &    0.6516    &   0.7057   &    0.4567     &   0.4948   \\
    $R^2$ of $M_2$  &    0.6043    &   0.7574   &    0.6082     &   0.7478   \\
    Difference      &    0.0472    &   0.0517   &    0.1516     &   0.2530   \\
    Optimal Weight  &     0.59     &    0.32    &     0.18      &    0.04    \\
    Puzzle          &     Yes      &     No     &      No       &     No     \\
    Sample Size     &     100      &    100     &     1000      &    1000    \\
    \bottomrule
    \end{tabular}
  \caption{``Difference`` represents the absolute difference of in-sample fit between two models. ``Optimal Weight`` is the estimated weight assigned to $M_1$. ``Puzzle`` indicates whether the simple average is close to or outperforms the optimal forecast combination.}
  \label{tab:bmag}
\end{table}

With reference to the previous results, when the absolute difference is small, the optimal weight $\omega_{opt}$ is expected to be around 0.5 and we are expected to find the puzzle. The second column of Table \ref{tab:bmag} provides another empirical evidence where the absolute difference is around 0.0472. The other three cases, however, illustrate the results when the absolute difference of $R^2$ is big enough. Different from the cases in the second and third columns of Table \ref{tab:size}, the puzzle is not obvious when one model is more favored, and we have the optimal forecast combination outperforms the simple average forecast. Recall our initial conjecture about the combination of a ``Good`` model and a ``Bad`` model, simulations have shown some corroborating evidence that the puzzle is ambiguous.



Table \ref{tab:bsig} further justifies our conjecture of the relationship between the in-sample performance and the presence of the puzzle. Especially when the sample size is 100, there is a huge difference between the in-sample fit of two models and $M_2$ is given all the weight in the optimal combination. This clearly implies that the puzzle is not discovered randomly but related to the model in-sample performance. It is also noticeable that conditioning on the same magnitude, the sample size has a large impact on the model fit. When the sample size is small, the absolute difference of in-sample performance becomes larger, leading to an extreme optimal weight and the presence of the puzzle is uncertain as well.

\begin{table}[ht]
  \centering
    \begin{tabular}{l|cccc}
    \toprule
    Different Signs &  $\beta_1=2,\ \beta_2=-2$  &  $\beta_1=4,\ \beta_2=-4$  &  $\beta_1=2,\ \beta_2=-2$  &  $\beta_1=4,\ \beta_2=-4$\\
    \midrule
    $R^2$ of $M_1$  &    0.0002    &   0.00002  &    0.0131     &   0.0423   \\
    $R^2$ of $M_2$  &    0.1130    &   0.1934   &    0.0321     &   0.0856   \\
    Difference      &    0.1128    &   0.1934   &    0.0191     &   0.0433   \\
    Optimal Weight  &      0       &     0      &     0.38      &    0.38    \\
    Puzzle          &      No      &     No     &      Yes      &    Yes     \\
    Sample Size     &     100      &    100     &     1000      &    1000    \\
    \bottomrule
    \end{tabular}
  \caption{``Difference`` represents the absolute difference of in-sample fit between two models. ``Optimal Weight`` is the estimated weight assigned to $M_1$. ``Puzzle`` indicates whether the simple average is close to or outperforms the optimal forecast combination.}
  \label{tab:bsig}
\end{table}





+ \bf{Variance of regressors}

We keep the variance of $x_{2i}$ the same value and only increase the variance of $x_{1i}$. Then $x_{1i}$ should have a larger variance than $x_{2i}$, thus the variation of $y_i$ can be explained more by Model 1 than Model 2. This can be verified by Table \ref{tab:regvar} where $R^2$ of $M_1$ is always higher than that of $M_2$. Consequently, the in-sample performance difference between the two models is big enough to presume that all four combinations include a ``good`` Model 1 and a ``bad`` Model 2. As expected in the conjecture, Model 1 should have a higher weight, far away from 0.5, in the optimal combination. Furthermore, the forecast combination puzzle is evidenced in three of them while it is not found in the last situation, indicating that the presence of the puzzle is unclear when there is a big gap in the in-sample fit.

\begin{table}[ht]
  \centering
    \begin{tabular}{l|cccc}
    \toprule
    Change in Variance of $x_{1i}$    &  $Var(x_{1i}) = 2$   &  $Var(x_{1i}) = 4$  &  $Var(x_{1i}) = 2$  &  $Var(x_{1i}) = 4$  \\
    \midrule
    $R^2$ of $M_1$  &    0.5389    &   0.6056   &    0.3981     &   0.4947   \\
    $R^2$ of $M_2$  &    0.2899    &   0.2464   &    0.3225     &   0.2536   \\
    Difference      &    0.2490    &   0.3592   &    0.0756     &   0.2411   \\
    Optimal Weight  &     0.92     &    0.94    &     0.66      &    0.85    \\
    Puzzle          &      Yes     &    Yes     &      Yes      &     No     \\
    Sample Size     &     100      &    100     &     1000      &    1000    \\
    \bottomrule
    \end{tabular}
  \caption{``Difference`` represents the absolute difference of in-sample fit between two models. ``Optimal Weight`` is the estimated weight assigned to $M_1$. ``Puzzle`` indicates whether the simple average is close to or outperforms the optimal forecast combination.}
  \label{tab:regvar}
\end{table}



One additional hypothesis is that when the absolute difference of $R^2$ between two models is less than 0.05, they should be treated as having similar in-sample performance. **Formally, the null hypothesis is that the absolute difference of the in-sample fit $R^2$ is less than 0.05.**

These results provide a general idea of the relationship between the in-sample fit of constituent models and the presence of the forecast combination puzzle. Based on the new information, the conjecture for two-model pools should be updated, as illustrated in Table \ref{tab:2}.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
Absolute difference of in-sample fit    &     Small      &    Large    \\
Presence of the puzzle                  &     $\surd$    &    $?$      \\
\end{tabular}
\caption{``Small`` means that both models fit the in-sample data equally well (or equally bad), whereas ``Large`` implies that one of the models performs poorly in fitting the training set. The ``$\surd$`` implies the presence of the forecast combination puzzle, while ``$?$`` means that the presence of the puzzle is ambiguous.}
\label{tab:2}
\end{table}


The choice of model is arbitrary and only the two-model pool is considered. It is also not prudent to determine ``small`` and ``Large`` difference based on subjective opinions. Potential improvements include...








