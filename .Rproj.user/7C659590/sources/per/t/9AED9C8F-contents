---
title: "Research Reference"
author: "30204232 Xiefei (Sapphire) Li"
date: "`r Sys.Date()`"
output: prettydoc::html_pretty
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Optimal Prediction Pools (Geweke & Amisano, 2011)  

Replication

+ Split the data proportionally into the training and test sets.  

+ Estimate two suitable models with the training set.  

+ Evaluate/Calculate the log predictive score for each model with the estimated parameters and test data set.  

+ Calculate the weight that maximizes the log predictive socre function of the linear combination.  


Data: S&P500  

Training: $t$  

Test: $T$  


Models: ARMA(1,1) and ETS(M,A,N)  


ARMA(1,1)  

$$y_t = \phi_0 + \phi_1y_{t-1} + \theta_1\epsilon_{t-1} + \epsilon_t \ \ \ \ \epsilon_t \stackrel{i.i.d.}{\sim} N(0,\sigma^2)$$  


<p>&nbsp;</p>  


In order to obtain forecasts for future values, someone requires conditional mean.

+ Conditional Mean  

Based on the model assumption, we have $E[\epsilon_t] = E[\epsilon_{t-1}] = 0$, $Var[\epsilon_t] = Var[\epsilon_{t-1}] = \sigma^2$ and $\epsilon_t$ is independent to $\epsilon_{t-1}$

$E[\epsilon_{t-1}^2] = Var[\epsilon_{t-1}] + E[\epsilon_{t-1}] = Var[\epsilon_{t-1}] = \sigma^2$

$$\begin{aligned}
E[Y_t|y_{t-1}] &= E[\phi_0 + \phi_1y_{t-1} + \theta_1\epsilon_{t-1} + \epsilon_t | y_{t-1}] \\ 
&= \phi_0 + \phi_1y_{t-1} + \theta_1\epsilon_{t-1}
\end{aligned}$$  

+ Conditional (Unconditional) Variance  

$$\begin{aligned}
Var[Y_t|y_{t-1}] &= Var[Y_t] \\
&= Var[\phi_0 + \phi_1Y_{t-1} + \theta_1\epsilon_{t-1} + \epsilon_t] \\
&= \phi_1^2 Var[Y_{t-1}] + \theta_1^2 Var[\epsilon_{t-1}] + 2\phi_1\theta_1 Cov(y_{t-1},\epsilon_{t-1}) + Var[\epsilon_t] + 2\phi_1 Cov(y_{t-1},\epsilon_{t}) \\
&= \phi_1^2 Var[Y_{t-1}] + \theta_1^2\sigma^2 + 2\phi_1\theta_1\sigma^2 + \sigma^2
\end{aligned}$$  

$Var[Y_t] = \frac{\theta_1^2\sigma^2 + 2\phi_1\theta_1\sigma^2 + \sigma^2}{1-\phi_1^2} = \frac{(\theta_1^2 + 2\phi_1\theta_1 + 1)\sigma^2}{1-\phi_1^2}$


$$\begin{aligned}
Cov(y_{t-1},\epsilon_{t-1}) &= E[y_{t-1}\epsilon_{t-1}]-E[y_{t-1}]E[\epsilon_{t-1}] \\
&= E[(\phi_0 + \phi_1y_{t-2} + \theta_1\epsilon_{t-2} + \epsilon_{t-1}) \epsilon_{t-1}] \\
&= \phi_0E[\epsilon_{t-1}] + \phi_1E[y_{t-2}]E[\epsilon_{t-1}] + \theta_1E[\epsilon_{t-2}]E[\epsilon_{t-1}] + E[\epsilon_{t-1}^2] \\
&= \sigma^2
\end{aligned}$$  


Since the error term ($\epsilon_t$) follows a Gaussian distribution, $y_t$ also follows a normal probability density function.  

<p>&nbsp;</p>  

Set $\sigma^2_y = \frac{(\theta_1^2 + 2\phi_1\theta_1 + 1)\sigma^2}{1-\phi_1^2}$ for simplicity.

The probability density function for the future value of y, for example $y_{t+1}$ will be  

$$f(y_{t+1}|y_{1:t})=f(y_{t+1}|y_t) = N\Big[ y_{t+1} | \phi_0 + \phi_1y_t + \theta_1\epsilon_t, \sigma^2_y \Big]$$  

These unknown parameters can be replaced by their estimates, obtained by fitting the model with the training data.  

The log predictive score function is defined as (reference)

$$LS = L_{1:T} = \frac{1}{T} \sum^T_{j=t+1}l_j = \frac{1}{T} \sum^T_{j=t+1} log \  N\Big[ y^{obs}_j | \hat\phi_0 + \hat\phi_1y^{obs}_{j-1} + \hat\theta_1\tilde\epsilon_{j-1}, \sigma^2_y \Big]$$

$\tilde\epsilon_{t}$ can be retrived from the model estimation
For $\tilde\epsilon_{t+k}, k = 1,2,...,T$, they can be calculated through 

$$\tilde\epsilon_{t+k} = y^{obs}_{t+k} - \hat\phi_0 + \hat\phi_1y^{obs}_{t+k-1} + \hat\theta_1\tilde\epsilon_{t+k-1} $$





Geweke, J., & Amisano, G. (2011). Optimal prediction pools. Journal of Econometrics, 164(1), 130â€“141. https://doi.org/10.1016/j.jeconom.2011.02.017




$$\epsilon_t = \frac{y_t - (l_{t-1}+b_{t-1})}{l_{t-1}+b_{t-1}}$$




$$y_t = \beta_0 + \beta_1t + \epsilon_t \\
\epsilon_t = \phi_1 \epsilon_{t-1} + u_t$$  

$$(1-\phi_1B)\epsilon_t = u_t$$  


$$\begin{aligned}
(1-\phi_1B) y_t &= (1-\phi_1B) \beta_0 + (1-\phi_1B) \beta_1t + (1-\phi_1B) \epsilon_t \\
y_t - \phi_1 y_{t-1} &= \beta_0 - \phi_1 \beta_0 + \beta_1t - \phi_1\beta_1 t + \phi_1\beta_1 + u_t \\
y_t &= (\beta_0 - \phi_1 \beta_0 + \phi_1\beta_1)+ \phi_1 y_{t-1} + (\beta_1 - \phi_1\beta_1) t + u_t
\end{aligned}$$

$$y_2 = \beta_0 + 2\beta_1 + \phi_1 \epsilon_1 + u_2$$  



$$E(Y_t) = \beta_0 + \beta_1t + \phi_1 \epsilon_{t-1}$$



<!-- ARIMA(2,0,2)(0,1,1)[4] w/ drift  -->

$$\begin{aligned}
(1-\phi_1B-\phi_2B^2) (1-B^4) log(y_t) &= c + (1+\theta_1B+\theta_2B^2)(1+\Theta_1B)\epsilon_t \\
(1-\phi_1B-\phi_2B^2 - B^4+\phi_1B^5+\phi_2B^6) log(y_t) &= c+(1+\theta_1B+\theta_2B^2 + \Theta_1B+\theta_1\Theta_1B^2+\theta_2\Theta_1B^3)\epsilon_t \\
log(y_t) - \phi_1 log(y_{t-1})-\phi_2 log(y_{t-2}) - log(y_{t-4}) +\phi_1 log(y_{t-5}) +\phi_2 log(y_{t-6}) &= c+\epsilon_t + \theta_1 \epsilon_{t-1} +\theta_2 \epsilon_{t-2} + \Theta_1 \epsilon_{t-1} +\theta_1\Theta_1 \epsilon_{t-2} +\theta_2\Theta_1 \epsilon_{t-3}
\end{aligned}$$




$$\begin{aligned}
log(y_t) &= c + \phi_1 log(y_{t-1})+\phi_2 log(y_{t-2}) + log(y_{t-4}) -\phi_1 log(y_{t-5}) -\phi_2 log(y_{t-6}) + \epsilon_t + \theta_1 \epsilon_{t-1} +\theta_2 \epsilon_{t-2} + \Theta_1 \epsilon_{t-1} +\theta_1\Theta_1 \epsilon_{t-2} +\theta_2\Theta_1 \epsilon_{t-3} \\
&= c + \phi_1 log(y_{t-1})+\phi_2 log(y_{t-2}) + log(y_{t-4}) -\phi_1 log(y_{t-5}) -\phi_2 log(y_{t-6}) + \epsilon_t + (\theta_1+ \Theta_1) \epsilon_{t-1} + (\theta_2+\theta_1\Theta_1) \epsilon_{t-2} +\theta_2\Theta_1 \epsilon_{t-3} \\
\end{aligned}$$





























