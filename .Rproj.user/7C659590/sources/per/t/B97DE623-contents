---
chapter: 2
knit: "bookdown::render_book"
---

# Methodology

The first goal is to construct linear density forecast combinations with two parametric models, selected from several possible models, for a given data. On top of point forecasts, using density forecast can benefit forecasters or decision markers with a broader view of understanding the target variable and potential risks (see the section 2.6.1. of @FTP22 for related contributions). The weighting scheme is to maximize the log predictive score function, which is comprised of two selected forecast densities. The procedure refers to the two-step approach mentioned before. The results should not be surprising that some forecast combinations will indeed improve the forecast accuracy via assessing the log predictive score function.

Next goal is to estimate unknown parameters of constituent models and the weight in a single step. This one-step approach is expected to have a better performance than the two-step approach stated above by conducting the forecast accuracy test.

Before explaining further details, the following notations will be used throughout the paper. The dataset with $T$ observations will be divided proportionally into two parts, an in-sample period $R$ and an out-of-sample period $P$. The realization of a target variable $y$ at time $t$ is denoted as $y_{t}$. Its future value after the in-sample period is denoted as $y_{\small{R+h}}$ where $h$ is the forecast horizon. $\mathcal{F}_t$, the information set at time t, is consists of all observed (and known) realizations of $y$ up to time t, i.e., $\mathcal{F}_t = \{y_1, y_2, .., y_t\}$.



## Forecast Combination Method

For the first step, I will estimate unknown parameter of each constituent model by the Maximum Likelihood Estimation. Estimates will then be held fixed and substituted into their corresponding probability density function.

With the idea of linear pooling [@BG69;@HM07;@GA11], the linear combinations of two predictive densities $f^{(t)}$ will be constructed with two constituent predictive densities $f^{(t)}_1$ and $f^{(t)}_2$:  

\begin{equation}
f^{(t)}(y) = wf^{(t)}_1(y) + (1-w)f^{(t)}_2(y)
\end{equation}  

where $f^{(t)}_1(y)$ and $f^{(t)}_2(y)$ are assumed to follow the normal distributions but with different means and variances, $h$ is the future value after the in-sample period ($R$), and $w$ is the weight allocated to the first model. Through this construction, the sum of two weights is implied to be 1, which is necessary and sufficient for the combination to be a density function[@GA11].

More specifically, $f^{(t)}_1(y)=f_1(y_t|\mathcal{F}_{t-1})=N\{y_t; \mu_1, \sigma^2_1\}$ and $f^{(t)}_2(y)=f_2(y_t|\mathcal{F}_{t-1})=N\{y_t; \mu_2, \sigma^2_2\}$. $N\{x; \mu, \sigma^2\}$ denotes the normal probability density function evaluated at value $x$ with mean $\mu$ and variance $\sigma^2$. Given $\mathcal{F}_{t-1}$, the conditional mean and conditional variance should be used.



## Evaluation of Models and Weighted Forecast Combinations {#evaluation}

This refers to the second step where I estimate the weight on the first model given the aforementioned estimates for parameters. The assessment of out-of-sample predictions for individual model and combinations will rely on the average log predictive score function.

The average log predictive score function of a specific model over the forecast horizon $h=1,2,...,P$ (i.e., the out-of-sample period) is defined as follows:

\begin{equation}
LS = \frac{1}{P}\sum^P_{h=1}logf(y_{\small{R+h}}) = \frac{1}{P}\sum^P_{h=1} logf(y_{\small{R+h}}| \mathcal{F}_{\small{R+h-1}})
\end{equation}


The optimal weight $w*$ will be estimated by maximizing the average logarithmic predictive score function over the out-of-sample period:

\begin{equation}
\frac{1}{P}\sum^P_{h=1}log\Big[wf_1(y_{\small{R+h}}|\mathcal{F}_{\small{R+h-1}}) + (1-w)f_2(y_{\small{R+h}}|\mathcal{F}_{\small{R+h-1}})\Big]
\end{equation}  

The corresponding forecast density combination, given the optimal weight, will be referred to the optimal combination.


## A Motivating Example

### Data  

Reconsidering the example in section 3 of @GA11, I use the daily Standard and Poor's (S&P) 500 index from February 11, 2013 to February 10, 2023 (10 years in total) retrieved via the @SP500. Total 2519 ($T$) available observations are partitioned into two periods with rough proportion. The in-sample period contains the first 60% of the data ($R = 1511$), which is used for estimating unknown parameters in each model. The rest 40% ($P = 1008$) becomes the out-of-sample period for further evaluation.  



### Model Specification {#model}

For a simple illustration purpose, I use five prediction models to study the performance of two-model pools: 

1. Model 1: An ARIMA(1,1,1) model with an intercept for the natural logarithm of S&P 500. 
2. Model 2: An ETS(M,N,N) model for the S&P 500. 
3. Model 3: An ETS(M,A,N) model for the S&P 500. 

ARIMA is short for autoregressive integrated moving average, ETS stands for exponential smoothing. All error terms are assumed to be independent and normally distributed with mean zero and variance $\sigma_m^2 \ \text{for}\  m = 1,2,3$.

4. Model 4: A linear regression model for the S&P 500 with a trend regressor and errors, follow an ARIMA(1,0,0) process. 
5. Model 5: A linear regression model for the natural logarithm of S&P 500 with a trend regressor and errors follow an ARIMA(1,0,0) process. 

Both error terms in the ARIMA model are assumed to be independent and normally distributed with mean zero and variance $\sigma_m^2 \ \text{for}\  m = 4,5$.


All unknown parameters in each model are estimated by maximizing the likelihood function with the in-sample period data. Estimated values are held fixed for the density evaluations. For each model, I generate the predictive densities at every future time point of S&P 500 returns ($h=1,2,...,P$) given that all information before that point is known. In order to compare every pair of these models, the log of S&P 500 returns will be "back-transformed" by evaluating the log normal density function.


As a reference, detailed formulas and explanations of these models can be found in @fpp3. The formula of the conditional variance for the ETS models in this case is discussed in Chapter 6.3 of @HKOS08. All coding are performed using R Statistical Software (R version 4.2.1 (2022-06-23)). Packages used are `tidyverse` [@tidy19], `dplyr` [@dplyr23], and `fpp3` [@fpp23].









