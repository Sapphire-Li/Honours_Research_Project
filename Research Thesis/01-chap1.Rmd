---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction 

## Research Objective

This thesis aims to investigate the determinants behind, and evidence for the forecast combination puzzle in various domains. Especially when we can expect the puzzle to be in evidence and then to empirically examine a general solution to the forecast combination puzzle. The combination puzzle refers to the well-known empirical finding that an equally weighted combination of forecasts generally outperforms more sophisticated combination schemes. This phenomenon is often referenced in the point forecast combinations literature but it is also present in the literature on density forecast combinations. Starting with two different types of time series datasets, this paper empirically explores how the in-sample performance of the constituent models influences the presence of the puzzle. 

The empirical studies undertaken so far have focused more on pure time series settings, while there is little literature on the puzzle in the cross-sectional setting. A simulated study is designed to investigate the puzzle in the two-model pool under a regression analysis. Throughout, we measure the performance of density combinations via the log score function and use mean squared forecast error to assess the accuracy of point combinations. As an additional contribution, we will assess the veracity, and applicability, of a recently proposed solution to the forecast combination puzzle suggested in @ZMFP22 and @FZMP23.  





## Literature Review and Motivation

Forecast accuracy is of critical concern for forecasters and decision makers. With the evidence of dramatic improvements in forecast accuracy, forecast combinations has attracted wide attention and contributions in the literature, both theoretical and applied [@C89;@T06]. More importantly, this approach often has robust performance across various types of data, proved by numerous empirical results [@GA11]. @MACF82 carefully examined the forecast accuracy with a considerable amount of time series, and reported that forecast combinations perform better than individual models. Later, @SW98 claimed that the best-performing single forecast can be further improved by incorporating other forecasts, based on their empirical comparisons of different forecasting methods. The prominence of researchers also devote efforts on probabilistic forecasting to obtain more information about the uncertainty of the resulting forecast. Similar to point forecasts, researchers now found that density forecast combination outperforms individual density forecast [e.g., @HM07;@GA11].

Forecast combinations refer to the idea of combining multiple forecasts generated from different models, which was originally proposed in the seminal work of @BG69. The forecast combination methods, in general, involve producing forecasts from constituent models, and then combining them based on a rule or weighting scheme. Each scheme has different selection criteria for the "best" forecast combination and the corresponding weight value assigned to each model. This process can sometimes capture more meaningful characteristics of the true data generating process than using a single model, and allows us to combine the best features of different models within a single framework. Researchers have examined a variety of combination methods for both point and density forecasts over the past 50 years, see @WHLK22 for a modern literature review.

In most time series setting under which forecast combinations are employed, a striking empirical phenomenon is often observed, coined by @SW04, as the "forecast combination puzzle". The puzzle is encapsulated by the fact that "theoretically sophisticated weighting schemes should provide more benefits than the simple average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22]. In other words, complex weighting schemes are designed to improve in-sample accuracy, so these refined forecast combinations should perform better out-of-sample in theory. However, the mean of the contemporaneous forecasts appears to be more robust in practice than forecasts combined through complicated weighting schemes. This finding has been continuously reaffirmed by extensive literature reviews and papers [e.g., @C89; @SW98; @SW04; @SW09; @MSA18; @MSA20], and the simple averaging naturally becomes a benchmark. 



There are two possible explanations for the puzzle in the literature. One concentrates on the estimation uncertainty in combination weight [@SW98; @SW04; @SW09]. Complicated weighting schemes introduce variability and uncertainty when estimating parameters, whereas the simple averaging does not require any estimation. The higher average loss and instability in the study of @SW04 were a strong evidence of the inferior performance of sophisticated weighting schemes. On the other hand, @E11 and @CMVW16 explore the trade-off between bias and variance in the Mean Squared Forecast Error (MSFE). @CMVW16 demonstrated the presence of bias and inefficiency when weights estimation is required, in comparison with the fixed-weights such as the equal weights. They further proved that equally weighted combination is unbiased and its variance has only one component, resulting in a smaller mean squared error than a biased combination. However, this is applicable and specific to the MSFE scheme. Furthermore, the underlying implication or condition of these explanations is that the puzzle must be found in each particular circumstance, which has not been rigorously demonstrated in theory. As a result, we will take a step back and focus on systematically exploring when the puzzle will be evidenced. In other words, when should we expect the puzzle to appear? 

Consider a simple case of two-model combination, the initial conjecture is that the presence of the puzzle is highly related to the in-sample fit of two constituent models. When both models fit the in-sample data well or bad, the puzzle is likely to happen. Intuitively, if the two models have comparable in-sample performance, then their forecasts will not differ much, and therefore the mean of two forecasts will not change much. More importantly, the forecast variance will be halved since there is no estimation required for simple average method. On the other hand, the optimal combination weight will not differ much from 0.5 because two models provide similar accuracy according to the in-sample fit. Besides, the sophisticated weighting scheme often involves an extra parameter estimation, which may introduce more variance and cannot be offset by its superior forecast accuracy. Therefore, the simple method is likely to outperform and we fall into the forecast combination puzzle. 

If only one of the models fails to capture the data patterns, the optimal forecast combination will give more weight to the better one. However, it is ambiguous as to whether or not the puzzle will be in evidence, with the simple average forecast performing much better than the optimal combination forecast, or vice versa. A good model that fits the in-sample period outstandingly can either continue maintaining its excellent performance or have an inverse impact on the forecast accuracy over the out-of-sample period. Actual data are highly flexible and models with unchanging structure are unlikely to adapt to any changing characteristics in the future. Table \ref{tab:1} summarizes this hypothesis.

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
                       &      & \multicolumn{2}{c}{$M_2$} \\
                       &      & Good       & Bad       \\
\multirow{2}{*}{$M_1$} & Good & $\surd$    & $?$ \\
                       & Bad  & $?$        & $\surd$
\end{tabular}
\caption{The first row and the first column refer to two constituent models in a combination, $M_1$ and $M_2$. ``Good`` means that the model fits the data well, whereas ``Bad`` denotes that the model fails capture some important features of the data. The ``$\surd$`` implies the presense of the forecast combination puzzle, while ``$?$`` means the forecast combination puzzle is uncertain.}
\label{tab:1}
\end{table}

More specifically, when the accuracy of in-sample fit for two models are very similar, then two models are both ``Good`` or both ``Bad``, which are the diagonal cases in Table \ref{tab:1}. It is possible that two models perform equally bad but for different reasons. In terms of the off-diagonal cases, a ``Good`` model fit and a ``Bad`` model fit are determined in a relative sense, i.e., one model fits the data far more superior than the other. This can be indicated by the apparent difference in the in-sample accuracy between two models.

Even though there is a widespread literature among different pure time series settings, no attention appears to have been given to the cross-sectional setting. We investigate the forecasting performance of two-model pools for cross-sectional data in using simple linear regression models in a simulation study. We find evidence that the forecast combination puzzle is not just about the fit of constituent models but the interaction of the model with the true DGP. Elements that influence the relationship are the sample size, the true value of parameters, and the variances of regressors. The advantages of using simulation are that the true DGP is known and we can easily manipulate values to see any interesting changes. This study also provides sufficient empirical evidence to better examine or support our conjecture in Table \ref{tab:1}. In addition, the regression form is easy to understand and analyse how various parts interact with each other. For example, it is known that the in-sample fit of a linear regression model can be represented by $R^2$, which is therefore a suitable measure to determine ``Good`` and ``Bad`` models in this context.



While various explanations for the forecast combination puzzle have been suggested over the years (see the above references), a general solution to the puzzle has so far proved elusive. Recently, @ZMFP22 and @FZMP23 proposed a new explanation for the puzzle in a general way by investigating the sampling variability of the forecasts induced via estimation of the constituent model forecasts (i.e., the models used to produce the forecasts). They illustrated that, asymptotically, the bias and variability mainly come from the estimation of the models used to produce the constituent model forecasts. The common way of producing forecast combinations keeps the model estimation uncertainty fixed during the weight estimation process, which is one reason of having the puzzle. @FZMP23 show that if constituent models and weights can be estimated jointly, the puzzle can be eliminated. Under this approach, the sophisticated weighting schemes should (asymptotically) be superior.

The goal of this thesis is two-fold: first, to substantiate the presence of the combination puzzle in the usual time series in which it has been found; second, to explore the relationship between the puzzle and the in-sample fit of constituent models; third, to search for empirical evidence of the combination puzzle in cross-sectional settings; fourth, to test the empirical veracity of the theoretical solution to the puzzle found in @FZMP23, both within, and outside of, the standard time series setting where the puzzle is often observed.







