---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction 

## Research Objective

This thesis aims to investigate the determinants behind, and evidence for the forecast combination puzzle in various domains. The combination puzzle refers to the well-known empirical finding that an equally weighted combination of forecasts generally outperforms more sophisticated combination schemes. While this phenomenon is often referenced in the point forecast combinations literature, it is also present in the literature on density forecast combinations. Starting with two different types of time series datasets, several two-model pools are constructed to explore how the presence of the puzzle relates to the in-sample performance of the constituent models used to produce the combination. 

The empirical studies undertaken so far have focused more on pure time series settings, while there is little literature on the puzzle in the cross-sectional setting. A simulated study is designed to investigate the puzzle in the two-model pool under a regression analysis. In addition, we derive and obtain a closed-form expression that supports this finding in the linear regression case. Throughout, we measure the performance of density combinations via the log score function and use mean squared forecast error to assess the accuracy of point combinations.





## Literature Review and Motivation

Forecast accuracy is of critical concern to forecasters and decision makers. The application of forecast combination, originally proposed in the seminal work of @BG69, provides improvements in forecast accuracy relative to individual forecasts, and therefore has attracted wide attention and contributions in the literature, both theoretical and applied [@C89;@T06]. More importantly, this approach often has robust performance across various types of data, proved by numerous empirical results [@GA11]. Many researchers also devote efforts on probabilistic forecasting to obtain more information about the uncertainty of the resulting forecast. Similar to point forecasts, researchers have found that density forecast combination outperform individual density forecast [e.g., @HM07;@GA11].



Forecast combination methods, in general, involve combining multiple forecasts generated from individual or constituent models based on a rule or weighting scheme. Every scheme has its own objective function for producing the "best" forecast combination, along with the optimal weight assigned to each model. This process can sometimes capture more meaningful characteristics of the true data generating process than using a single model, and allows us to combine the best features of different models within a single framework. Researchers have examined a variety of combination methods for both point and density forecasts over the past 50 years, see @WHLK22 for a modern literature review.

In most time series setting under which forecast combinations are employed, a striking empirical phenomenon is often observed, coined by @SW04, as the "forecast combination puzzle". The puzzle is encapsulated by the fact that "theoretically sophisticated weighting schemes should provide more benefits than the simple average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22]. In other words, complex weighting schemes designed to improve in-sample accuracy should in theory perform better out-of-sample. However, the mean of the constituent forecasts appears to be more robust in practice than forecasts combined through complicated weighting schemes. This finding has been continuously reaffirmed by extensive literature reviews and papers [e.g., @MACF82; @C89; @MSA18; @MSA20], and the simple averaging naturally becomes a benchmark. For the purposes of this analysis, we explicitly define the forecast combination puzzle as: 1) the simple average has better out-of-sample performance than that of the optimal combination; and 2) the forecast accuracy between the optimal combination and the simple average being small, which allows for the optimal combination to be slightly higher than the simple average. As the forecast accuracy is not too different, using either combination method will not make a meaningful difference, except in special circumstances.



The literature explains the puzzle mainly in three aspects: the estimation uncertainty in complicated weighting schemes [@SW98; @SW04; @SW09], the bias and inefficiency in the Mean Squared Forecast Error (MSFE) function [@E11; @CMVW16], and the sampling variability of the forecasts induced via estimation of the constituent model forecasts [@ZMFP22; @FZMP23]. However, all of these explanations implicitly assume that the puzzle will be in evidence when combining forecasts, regardless of the choice of constituent models or the weighing scheme. They overlook the possibility that complicated combination methods can perform better than the simple average in some cases. In order to make a rigorous explanation statement, we systematically explore the determinants behind the presence of the puzzle. Even though there is a widespread literature among different pure time series settings, no attention appears to have been given to the cross-sectional setting. Therefore, we will investigate the puzzle in both time series and cross-sectional settings using empirical and simulated data respectively.



Considering a simple case of two-model combination, our initial conjecture is that the presence of the puzzle is tightly-related to the in-sample fit of two constituent models. We conjecture that when constituent models have similar in-sample fit, the puzzle will be in evidence. Otherwise, the presence of the puzzle is uncertain. Intuitively, the model in-sample performance greatly affects the behavior of forecasts, so forecasts produced by two similarly performed models will not differ much, leading to an estimated optimal weight around one-half. Consequently, we should expect small differences in forecast accuracy between optimally-combined forecast and equally-combined forecast. It is then reasonable to prefer the simple average given that the forecast variance will also be lessened due to no extra parameter estimation. On the contrary, if two models have distinct in-sample fit, we conjecture that the optimal forecast combination will give more weight to the better-performing forecast and therefore the estimated weights will be far away from a half. The estimated value of the optimal weight does not necessarily indicate the out-of-sample forecast accuracy of the optimal combination. Hence, there are two possible situations: the equally-combined forecast perform better than the optimally-combined forecast, and the opposite. The presence of the puzzle becomes ambiguous, depending on the situation we fall into. 

According to the definition of the puzzle, our conjecture can be summarized in Table \ref{tab:1}. Two constituent models are evaluated based on their in-sample relative performance and are also allowed to perform equivalently ``Bad`` for different reasons.

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
                       &      & \multicolumn{2}{c}{$M_2$} \\
                       &      & Good       & Bad       \\
\multirow{2}{*}{$M_1$} & Good & $\surd$    & $?$ \\
                       & Bad  & $?$        & $\surd$
\end{tabular}
\caption{The first row and the first column refer to two constituent models in a combination, $M_1$ and $M_2$. ``Good`` means that the model fits the data well, whereas ``Bad`` denotes that the model fails to capture some important features of the data. The ``$\surd$`` indicates the presence of the forecast combination puzzle, while ``$?$`` implies that the presence of the puzzle is uncertain.}
\label{tab:1}
\end{table}



We demonstrate that the forecast combination puzzle is in evidence in the time series setting with the S\&P500 index and the quarterly unemployment data. That is, the equally-weighted combination provides equivalent or superior forecast accuracy relative to the optimally-weighted combination. We then compare the in-sample fit of constituent models using their in-sample log likelihood and validate most of our conjecture. 

Furthermore, we investigate the forecasting performance of two-model pools for simulated cross-sectional data using simple linear regression models. We derive a mathematical relationship between the optimal combination weight under the mean squared forecast error and elements in the true DGP. This relationship implies that the forecast combination puzzle is tightly-related to the interaction between constituent models and the true DGP. Given this knowledge in the point combination setting, we empirically investigate this finding for density combinations and validate that these formal reasoning are applicable in a more general setting. In addition, this simulation study provides sufficient empirical evidence to examine conjecture.



The goal of this thesis is two-fold: first, to substantiate the presence of the combination puzzle in the time series setting and to explore the relationship between the puzzle and the in-sample fit of constituent models; second, to mathematically derive the formula of the optimal weight in the regression setting under mean squared forecast error and then to validate the conjecture in Table \ref{tab:1} with empirical evidence.

The thesis follows two common weighting schemes for `two model` pools (Section 2) and then applies the log score to density combinations for daily S\&P 500 index and the mean squared error to point combinations for quarterly number of unemployed (Section 3). With some empirical evidence of the conjecture, Section 4 derives a closed-form expression for the optimal weight under the mean squared error in a simple regression case. The findings are further examined by analyzing density combinations in the cross-sectional setting. The final section concludes.



