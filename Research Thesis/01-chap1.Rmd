---
chapter: 1
knit: "bookdown::render_book"
---

# Introduction 

## Research Objective

This thesis aims to investigate the determinants behind, and evidence for the forecast combination puzzle in various domains, and to empirically examine a general solution to the forecast combination puzzle. The combination puzzle refers to the well-known empirical finding that an equally weighted combination of forecasts generally outperforms more sophisticated combination schemes. This phenomenon is often found in the point forecast combinations but it is also the case in the density forecast combinations. Starting with time series data, this paper explores how the puzzle is affected by the fitness of models on the high volatile or the strong seasonal dataset. The empirical studies undertaken so far have focused more on pure time series settings, while there is little literature on the presence of the combination puzzle in the cross-sectional setting. A simulated study is conducted to study the puzzle in the two-model combination under a regression analysis. The performance of density combinations will be assessed via the log score function and mean squared forecast error is used to assess the point combinations for seasonal data. As an additional contribution, we will assess the veracity, and applicability, of a recently proposed solution to the forecast combination puzzle suggested in @ZMFP22 and @FZMP23.  





## Literature Review and Motivation

The forecast accuracy is of critical concern for forecasters and decision makers. With the evidence of dramatic improvements in the forecast accuracy, forecast combinations have attracted wide attention and contributions in the literature, both theoretical and applied [@C89;@T06]. More importantly, this promising approach often has a robust performance for various types of series, which is borne out by numerous empirical results [@GA11]. @MACF82 carefully examined the forecast accuracy with a considerable amount of time series, and reported that forecast combinations perform better than individual models. Later, @SW98 claimed that the best-performing single method can be further improved by incorporating other forecasts, based on empirical comparisons of different forecasting methods. Despite of the point forecasting, researchers also devote efforts on probabilistic forecasting with more information about uncertainties and continue to find that optimal density forecast combination outperforms individual forecasts [e.g., @HM07;@GA11].

Forecast combinations refer to the idea of combining multiple forecasts generated from possible models, which was originally proposed in the seminal work of @BG69. The forecast combination methods, in general, involve producing forecasts from constituent models, and then combining them based on a rule or weighting scheme. Each scheme has different selection criteria for the "best" forecast combination and the corresponding weight value assigned to each model. This process can sometimes capture more meaningful characteristics of the true data generating process than using a single model, and allow us to combine the best features of different models within a single framework. Researchers have examined a variety of combination methods for both point and density forecasts over the past 50 years, see @WHLK22 for a modern literature review.

In most time series setting under which forecast combinations are employed, a striking empirical phenomenon is often observed, coined by @SW04, as the "forecast combination puzzle". The puzzle is encapsulated by the fact that "theoretically sophisticated weighting schemes should provide more benefits than the simple average from forecast combination, while empirically the simple average has been continuously found to dominate more complicated approaches to combining forecasts" [@WHLK22]. In other words, complex weighting schemes are designed to improve the accuracy, so these refined forecast combinations should perform better in theory. However, the mean of the contemporaneous forecasts appears to be more robust in practice than weighted forecasts combined through complicated schemes. This finding has been reaffirmed by extensive literature reviews and papers [e.g., @C89; @SW98; @SW04; @SW09; @MSA18; @MSA20], and simple averaging naturally becomes a benchmark. 



There are two possible explanations for the puzzle in the literature. One concentrates on the estimation uncertainty in combination weight [@SW98; @SW04; @SW09]. Complicated weighting schemes introduce variability and uncertainty when estimating parameters, whereas the simple averaging does not require any estimation. The higher average loss and instability in the study of @SW04 were a strong evidence for the inferior performance of sophisticated weighting schemes. On the other hand, @E11 and @CMVW16 explore the trade-off between bias and variance in the Mean Squared Forecast Error (MSFE). @CMVW16 demonstrated the presence of bias and inefficiency when weights estimation is required, in comparison with the fixed-weights such as the equal weights. They further proved that equally weighted combination is unbiased and its variance has only one component, resulting in a smaller mean squared error than a biased combination. However, this is applicable and specific to the MSFE scheme. 

While various explanations for the forecast combination puzzle have been suggested over the years (see the above references), a general solution to the puzzle has so far proved elusive. Recently, @ZMFP22 and @FZMP23 proposed a new explanation for the puzzle in a general way by investigating the sampling variability of the forecasts induced via estimation of the constituent model forecasts (i.e., the models used to produce the forecasts). They illustrated that, asymptotically, the bias and variability mainly come from the estimation of the models used to produce the constituent model forecasts. The common way of producing forecast combinations keeps the model estimation uncertainty fixed during the weight estimation process, which is one reason of having the puzzle. If constituent models and weights can be estimated jointly, if feasible, the puzzle can be eliminated suggested by @FZMP23. Under this approach, the sophisticated weighting schemes should (asymptotically) be superior.



Following the research direction of @BS16, we are looking for a relationship between model fitting and the presence of the puzzle in the case of two-model combination. We speculate that the puzzle is only revealed when both constituent models are correctly specified and fit the data well. If one of them fails to capture the data patterns, the forecast combination will give more weight to the better model. Even if both models are misspecified, the complicated weighing scheme will try to reduce the uncertainty of the worse model as much as possible. Table \ref{tab:1} visually summarizes the conjectures.


\begin{table}[ht]
\centering
\begin{tabular}{cccc}
                       &      & \multicolumn{2}{l}{$M_2$} \\
                       &      & Good       & Bad       \\
\multirow{2}{*}{$M_1$} & Good & $\surd$    & $\times$ \\
                       & Bad  & $\times$   & $\times$
\end{tabular}
\caption{The first row and the first column refer to two constituent models in a combination, $M_1$ and $M_2$. Good indicates the model is correctly specified and fits the data well, whereas Bad denotes the model is misspecified and fails capture the important features of the data. The ``$\surd$`` implies the presense of the forecast combination puzzle, while ``$\times$`` means no forecast combination puzzle.}
\label{tab:1}
\end{table}



Even though there is a widespread literature among different pure time series settings, no attention appears to have been given to the cross-sectional setting. We investigate the forecasting performance of cross-sectional data and misspecified models via a simulation study. At a glance, the presence of the puzzle is determined by at least 3 elements in the two-model pools, which are the sample size, the true value of parameters except the intercept, and the variances of regressors.



The goal of this thesis is manifolds: first, to substantiate the presence of the combination puzzle in the usual time series in which it has been found; second, to examine the relationship between the puzzle and the model specification; third, to search for empirical evidence of the combination puzzle in cross-sectional settings; fourth, to test the empirical veracity of the theoretical solution to the puzzle found in @FZMP23, both within, and outside of, the standard time series setting where the puzzle is often observed.









