---
chapter: 
knit: "bookdown::render_book"
---

# Simulation Results

## Pure cross-sectional setting

Given that the forecast combination can greatly improve the forecast accuracy, this idea of model combination can also be applied to the cross-sectional setting. Rather than forecasting future value, cross-sectional data often helps to better understand the individual behavior and decision-making with changing attributes. 

A simulated cross-sectional dataset is designed to study how related elements in the linear regression model affect the presence of the puzzle, as well as the performance of density combinations. In line with previous notations but under the cross-sectional setting, the subscript `t` will change to `i` to represent each individual observation.


CHECK MOTIVATION!!!! 

### Experimental design

The true data-generating process (DGP) is assumed to be a classic linear regression model with only two exogenous and correlated regressors, which satisfies all classical assumptions:

\begin{equation}
\label{eqn:DGP}
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + e_i, \ \ e_i \stackrel{i.i.d}{\sim} N(\mu_e,\sigma^2_e) \\
\end{equation}

where $i$ represents each observation.  

The initial set-up has 15000 (N) artificial cross-sectional observations generated from \ref{eqn:DGP} with $E[x_{1i}] = E[x_{2i}] = 0$, $Var(x_{1i}) = Var(x_{2i}) = 1$, $Cov(x_{1i}, x_{2i}) = 0.7$, $\pmb{\beta} = (\beta_0, \beta_1, \beta_2)' = (1,2,2)'$, $\mu_e = 5$ and $\sigma^2_e=10$.

Following the methodology in Section \@ref(method), the data will be divided into an in-sample period (roughly 60%) for estimation and an out-of-sample period for accuracy evaluation. We propose two mis-specified models to generate density forecasts with each only contains one of the regressors. Assume Model 1 includes only $x_{1i}$ as the regressor and the other model, Model 2, includes only $x_{2i}$ as the regressor. The density forecast combinations will follow the construction of `two-model` pools and be evaluated by the log score functions. 


+ Sample size is $N=15000$

+ $E[x_{1i}] = E[x_{2i}] = 0$

+ $Var(x_{1i}) = Var(x_{2i}) = 1$

+ $Cov(x_{1i}, x_{2i}) = 0.7$

+ The true value of $\pmb{\beta} = (\beta_0, \beta_1, \beta_2)' = (1,2,2)'$

+ $\mu_e = 5$ and $\sigma^2_e=10$


\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{figures/Sample_Size_15000.pdf}
\caption{Two curves refer to the in-sample (left) and out-of-sample (right) performance of density combinations with artificial cross-sectional data under the initial set-up. The x-axis represents the weight assignment on the Model 1 and the y-axis indicates the log score for each density combination. The orange dot represents the optimal set of weights and the corresponding log predictive score in each case, while the blue dot indicates the forecast performance of the simple averaging method. The green dot, as a reference, refers to the maximum point of the out-of-sample curve.}
\label{fig:ss15000}
\end{figure}

Figure \ref{fig:ss15000} clearly reflects that when the sample size is large enough, the simple average of predicted densities, indicated by the blue dot, can retain the forecast accuracy with a small difference in the log predictive score, compared with the optimal combination indicated by the orange dot. This is an evidence of facing forecast combination puzzle. Given the puzzle, we can change the true value of relevant elements one at a time while holding all others constant, and then summarize the conditions under which the puzzle is likely to be evident.


+ \bf{Sample Size}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/Sample_Size_100-10000.png}
\caption{Three columns refer to cases when $N=100$, $N=1000$, and $N=10000$ respectively while keeping all others constant as the initial set-up. The upper graphs represent the in-sample combination performance and the bottom graphs represent the out-of-sample combination accuracy. The meanings of colored dots are the same as those in Figure \ref{fig:ss15000}.}
\label{fig:ss}
\end{figure}

First, it is noticeable that the performances of in-sample and out-of-sample combinations have completely different shapes or features when $N=100$ but are gradually similar when $N=1000$ and $N=10000$. In the $N=100$ case, we completely prefer Model 1 to fit the training set, however, the Model 1 becomes the worse choice for the test set. Although the averaged density forecast performs much better than the combination recommended by the optimal weight, the accuracy could still be of concern. Second, the orange dot is approaching to the green dot when the sample size increases. This implies that the model combination which fits the in-sample well does not necessarily generate better forecasts when the sample size is small. In this case, it is conservative to use averaged forecasts.


To be more rigorous, the puzzle becomes notable when the sample size is larger than 9000. 

When we have a small dataset, it is not representative of the whole population, so the model estimation involves more randomness and is highly influenced by potential outliers. There is also a possibility of overfitting when the training and test sets have distinct patterns.

Given a large enough dataset and two equally good models



+ \bf{Magnitude and Sign of $\pmb{\beta}$}

Next, the sample size is set to be 10000 so that it is large enough to reveal the puzzle. Consider the change in magnitude and sign of $\beta_1$ and $\beta_2$.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/Beta_diff_mag.png}
\caption{In this case, $\beta_1$ and $\beta_2$ have the same sign but different magnitudes. The first column refers to $\beta_1=2$ and $\beta_2=3$, the second column refers to $\beta_1=2$ and $\beta_2=4$, and the first column refers to $\beta_1=2$ and $\beta_2=6$.}
\label{fig:magnitude}
\end{figure}

The puzzle is highly sensitive to the absolute difference between two parameters. If the absolute difference is large enough, generally more than half of the smaller coefficient, it is hard to observe the puzzle and the optimal combination always wins.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.55]{figures/Beta_diff_sign.png}
\caption{In both cases, $\beta_1$ and $\beta_2$ have the same magnitude but different signs, i.e. $\beta_1=-\beta_2$. The first column considers the case when $\beta_1=2$ and $\beta_2=-2$ and the second column considers the case when $\beta_1=4$ and $\beta_2=-4$.}
\label{fig:sign}
\end{figure}

The puzzle is insensitive when $\beta_1$ and $\beta_2$ only have opposite signs. In both cases, the simple averaging forecast combination performs better than the optimal forecast combination. 

Meanwhile, two regressors have the same size effect

the accuracy of the optimal prediction combination can be improved by having higher absolute value of the coefficients. In other words, regressors should have direct and large impacts on the dependent variable. This claim is also substantiated


The magnitude of each coefficient acts like the weight of each regressor on the dependent variable. 
The one which includes the more important regressor will be more favoured with a higher weight in the forecast combination



+ \bf{Variance of regressors}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/x_var.png}
\caption{The first column refers to the case when }
\label{fig:xvar}
\end{figure}


larger variance, more variation to explain y, more favored


Variance of the error term will influence the log score value
Pattern is hard to determine


Estimates based on equation (2) need not satisfy, as noted in section 2.1, especially if the correlation between the competing forecast errors is large and positive, and we consider two possibilities. One is to use the observed point estimate; the second, following widespread practice, is to replace an estimate out- side this range by the nearest boundary value, 0 or 1 as appropriate. There are then four combined forecasts whose MSFEs are calculated over the last P observations: three weighted averages, in turn using and the observed and truncated, and the simple average using k=0.5. The estimation cost of each weighted average is expressed as the percentage increase in MSFE above that of the simple average,




















